{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title_header"
   },
   "source": [
    "# ProteinMCP ‚Äî Fitness Modeling Workflow\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/charlesxu90/ProteinMCP/blob/main/notebooks/ProteinMCP_Fitness_Modeling.ipynb)\n",
    "\n",
    "Build and compare protein fitness prediction models using multiple backbone architectures:\n",
    "\n",
    "| Model | Description |\n",
    "|-------|-------------|\n",
    "| **EV+OneHot** | Evolutionary couplings + one-hot encoding (PLMC) |\n",
    "| **ESM2-650M / ESM2-3B** | Meta's protein language models |\n",
    "| **ProtT5-XL / ProtAlbert** | ProtTrans transformer embeddings |\n",
    "\n",
    "Each model is trained with SVR, XGBoost, and KNN heads, then compared via 5-fold cross-validated Spearman correlation.\n",
    "\n",
    "**Links:** [GitHub](https://github.com/charlesxu90/ProteinMCP) ¬∑ [ESM](https://github.com/facebookresearch/esm) ¬∑ [ProtTrans](https://github.com/agemagician/ProtTrans) ¬∑ [PLMC](https://github.com/debbiemarkslab/plmc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "user_config",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üìã User Configuration\n",
    "#@markdown ### Protein Settings\n",
    "PROTEIN_NAME = \"TEVp_S219V\" #@param {type:\"string\"}\n",
    "use_example_data = True #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ### API Key (required)\n",
    "ANTHROPIC_API_KEY = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown If `use_example_data` is **False**, upload your own `wt.fasta` and `data.csv` below.\n",
    "\n",
    "import os\n",
    "\n",
    "# ---------- Validate API key ----------\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY is required. Get one at https://console.anthropic.com/\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "\n",
    "# ---------- Paths ----------\n",
    "REPO_DIR     = \"/content/ProteinMCP\"\n",
    "DATA_DIR     = f\"/content/data/{PROTEIN_NAME}\"\n",
    "RESULTS_DIR  = f\"/content/results/{PROTEIN_NAME}\"\n",
    "WT_FASTA     = f\"{DATA_DIR}/wt.fasta\"\n",
    "DATA_CSV     = f\"{DATA_DIR}/data.csv\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "if use_example_data:\n",
    "    print(f\"Will use bundled example data for {PROTEIN_NAME}\")\n",
    "else:\n",
    "    from google.colab import files\n",
    "    print(\"Upload wt.fasta and data.csv (must contain 'seq' and 'log_fitness' columns):\")\n",
    "    uploaded = files.upload()\n",
    "    for fname, content in uploaded.items():\n",
    "        with open(os.path.join(DATA_DIR, fname), \"wb\") as f:\n",
    "            f.write(content)\n",
    "    assert os.path.exists(WT_FASTA), f\"Missing {WT_FASTA} ‚Äî please upload wt.fasta\"\n",
    "    assert os.path.exists(DATA_CSV),  f\"Missing {DATA_CSV} ‚Äî please upload data.csv\"\n",
    "\n",
    "print(f\"\\nPROTEIN_NAME : {PROTEIN_NAME}\")\n",
    "print(f\"DATA_DIR     : {DATA_DIR}\")\n",
    "print(f\"RESULTS_DIR  : {RESULTS_DIR}\")\n",
    "print(f\"WT_FASTA     : {WT_FASTA}\")\n",
    "print(f\"DATA_CSV     : {DATA_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_conda",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üêç Install Conda / Mamba\n",
    "%%time\n",
    "import os\n",
    "\n",
    "CONDA_READY = \"/content/.conda_ready\"\n",
    "\n",
    "if os.path.exists(CONDA_READY):\n",
    "    print(\"Conda already installed ‚Äî skipping.\")\n",
    "else:\n",
    "    # Install Miniforge (same pattern as ColabFold)\n",
    "    os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\")\n",
    "    os.system(\"bash Miniforge3-Linux-x86_64.sh -bfp /usr/local\")\n",
    "    os.system(\"rm -f Miniforge3-Linux-x86_64.sh\")\n",
    "\n",
    "    # Make conda available in this session\n",
    "    os.environ[\"PATH\"] = \"/usr/local/bin:\" + os.environ[\"PATH\"]\n",
    "    os.environ[\"CONDA_PREFIX\"] = \"/usr/local\"\n",
    "\n",
    "    # Create protein-mcp conda env (in-place, reuse base)\n",
    "    os.system(\"mamba install -y -n base python=3.12 pip nodejs=20\")\n",
    "\n",
    "    open(CONDA_READY, \"w\").close()\n",
    "    print(\"Conda/Mamba installed successfully.\")\n",
    "\n",
    "# Ensure PATH is set for subsequent cells\n",
    "os.environ[\"PATH\"] = \"/usr/local/bin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_proteinmcp",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üì¶ Install ProteinMCP & Claude Code\n",
    "%%time\n",
    "import os\n",
    "\n",
    "PROTEINMCP_READY = \"/content/.proteinmcp_ready\"\n",
    "REPO_DIR = \"/content/ProteinMCP\"\n",
    "\n",
    "if os.path.exists(PROTEINMCP_READY):\n",
    "    print(\"ProteinMCP & Claude Code already installed ‚Äî skipping.\")\n",
    "else:\n",
    "    # Clone repo\n",
    "    if not os.path.isdir(REPO_DIR):\n",
    "        os.system(\"git clone https://github.com/charlesxu90/ProteinMCP.git /content/ProteinMCP\")\n",
    "\n",
    "    # Install ProteinMCP as editable package\n",
    "    os.system(f\"pip install -e {REPO_DIR}\")\n",
    "    os.system(f\"pip install -r {REPO_DIR}/requirements.txt\")\n",
    "\n",
    "    # Install Claude Code\n",
    "    os.system(\"npm install -g @anthropic-ai/claude-code\")\n",
    "\n",
    "    open(PROTEINMCP_READY, \"w\").close()\n",
    "    print(\"ProteinMCP & Claude Code installed successfully.\")\n",
    "\n",
    "# Set API key for Claude Code\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "print(f\"ANTHROPIC_API_KEY set (ends with ...{ANTHROPIC_API_KEY[-4:]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_skill",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üîß Install Fitness Modeling Skill & MCPs\n",
    "%%time\n",
    "import os\n",
    "\n",
    "SKILL_READY = \"/content/.skill_ready\"\n",
    "REPO_DIR = \"/content/ProteinMCP\"\n",
    "\n",
    "if os.path.exists(SKILL_READY):\n",
    "    print(\"Fitness modeling skill already installed ‚Äî skipping.\")\n",
    "else:\n",
    "    # Install the fitness_modeling skill (installs msa_mcp, plmc_mcp, ev_onehot_mcp, esm_mcp, prottrans_mcp)\n",
    "    os.system(f\"cd {REPO_DIR} && pskill install fitness_modeling\")\n",
    "\n",
    "    open(SKILL_READY, \"w\").close()\n",
    "    print(\"Fitness modeling skill installed.\")\n",
    "\n",
    "# Verify MCP status\n",
    "os.system(f\"cd {REPO_DIR} && pmcp status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step0_setup",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Step 0 ‚Äî Setup Results Directory\n",
    "import os, shutil\n",
    "\n",
    "REPO_DIR    = \"/content/ProteinMCP\"\n",
    "EXAMPLE_DIR = f\"{REPO_DIR}/examples/case1_fitness_modeling\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "if use_example_data:\n",
    "    # Copy bundled example data into DATA_DIR\n",
    "    for fname in [\"wt.fasta\", \"data.csv\"]:\n",
    "        src = os.path.join(EXAMPLE_DIR, fname)\n",
    "        dst = os.path.join(DATA_DIR, fname)\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copy2(src, dst)\n",
    "            print(f\"Copied {src} ‚Üí {dst}\")\n",
    "\n",
    "# Copy input files to RESULTS_DIR (needed by training tools)\n",
    "for fname in [\"wt.fasta\", \"data.csv\"]:\n",
    "    src = os.path.join(DATA_DIR, fname)\n",
    "    dst = os.path.join(RESULTS_DIR, fname)\n",
    "    if not os.path.exists(dst):\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"Copied {src} ‚Üí {dst}\")\n",
    "\n",
    "# Verify\n",
    "assert os.path.exists(f\"{RESULTS_DIR}/wt.fasta\"), \"wt.fasta missing in RESULTS_DIR\"\n",
    "assert os.path.exists(f\"{RESULTS_DIR}/data.csv\"),  \"data.csv missing in RESULTS_DIR\"\n",
    "print(f\"\\nResults directory ready: {RESULTS_DIR}\")\n",
    "print(f\"Files: {os.listdir(RESULTS_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step1_msa",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Step 1 ‚Äî Generate MSA\n",
    "%%time\n",
    "import os\n",
    "\n",
    "REPO_DIR = \"/content/ProteinMCP\"\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "Can you obtain the MSA for {PROTEIN_NAME} from {WT_FASTA} using msa mcp \\\n",
    "and save it to {RESULTS_DIR}/{PROTEIN_NAME}.a3m.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "\"\"\"\n",
    "\n",
    "cmd = (\n",
    "    f'cd {REPO_DIR} && claude -p \"{prompt}\" '\n",
    "    f'--allowedTools \"mcp__msa_mcp__generate_msa,Bash,Read,Write\"'\n",
    ")\n",
    "os.system(cmd)\n",
    "\n",
    "# Verify output\n",
    "msa_file = f\"{RESULTS_DIR}/{PROTEIN_NAME}.a3m\"\n",
    "assert os.path.exists(msa_file), f\"MSA file not found: {msa_file}\"\n",
    "print(f\"\\nMSA generated: {msa_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step2_plmc",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Step 2 ‚Äî Build PLMC Model\n",
    "%%time\n",
    "import os\n",
    "\n",
    "REPO_DIR = \"/content/ProteinMCP\"\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "I have created an a3m file in {RESULTS_DIR}/{PROTEIN_NAME}.a3m. \\\n",
    "Can you help build an EV model using plmc mcp and save it to {RESULTS_DIR}/plmc directory. \\\n",
    "The wild-type sequence is {WT_FASTA}.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "After building the model, create symlinks in {RESULTS_DIR}/plmc/:\n",
    "  ln -sf {PROTEIN_NAME}.model_params uniref100.model_params\n",
    "  ln -sf {PROTEIN_NAME}.EC uniref100.EC\n",
    "\"\"\"\n",
    "\n",
    "cmd = (\n",
    "    f'cd {REPO_DIR} && claude -p \"{prompt}\" '\n",
    "    f'--allowedTools \"mcp__plmc_mcp__plmc_convert_a3m_to_a2m,mcp__plmc_mcp__plmc_generate_model,Bash,Read,Write\"'\n",
    ")\n",
    "os.system(cmd)\n",
    "\n",
    "# Verify outputs\n",
    "plmc_dir = f\"{RESULTS_DIR}/plmc\"\n",
    "assert os.path.exists(f\"{plmc_dir}/uniref100.model_params\"), \"PLMC model_params symlink missing\"\n",
    "assert os.path.exists(f\"{plmc_dir}/uniref100.EC\"), \"PLMC EC symlink missing\"\n",
    "print(f\"\\nPLMC model built: {os.listdir(plmc_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step3_ev_onehot",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Step 3 ‚Äî Build EV+OneHot Model\n",
    "%%time\n",
    "import os\n",
    "\n",
    "REPO_DIR = \"/content/ProteinMCP\"\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "I have created a plmc model in directory {RESULTS_DIR}/plmc. \\\n",
    "Can you help build an EV+OneHot model using ev_onehot_mcp and save it to {RESULTS_DIR}/ directory. \\\n",
    "The wild-type sequence is {RESULTS_DIR}/wt.fasta, and the dataset is {RESULTS_DIR}/data.csv.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "\"\"\"\n",
    "\n",
    "cmd = (\n",
    "    f'cd {REPO_DIR} && claude -p \"{prompt}\" '\n",
    "    f'--allowedTools \"mcp__ev_onehot_mcp__ev_onehot_train_fitness_predictor,Bash,Read,Write\"'\n",
    ")\n",
    "os.system(cmd)\n",
    "\n",
    "# Verify output\n",
    "assert os.path.exists(f\"{RESULTS_DIR}/metrics_summary.csv\"), \"EV+OneHot metrics not found\"\n",
    "print(f\"\\nEV+OneHot model trained. Metrics:\")\n",
    "import pandas as pd\n",
    "print(pd.read_csv(f\"{RESULTS_DIR}/metrics_summary.csv\").to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step4_esm",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Step 4 ‚Äî Build ESM Models\n",
    "%%time\n",
    "import os\n",
    "\n",
    "REPO_DIR = \"/content/ProteinMCP\"\n",
    "\n",
    "# --- 4.1: ESM2-650M (extract embeddings + train svr/xgboost/knn) ---\n",
    "prompt_650m = f\"\"\"\\\n",
    "Can you help train ESM models for data in {RESULTS_DIR}/ and save them to \\\n",
    "{RESULTS_DIR}/esm2_650M_{{head_model}} using the esm mcp server with svr, xgboost, \\\n",
    "and knn as the head models.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "Obtain the embeddings if they are not created.\n",
    "\"\"\"\n",
    "\n",
    "cmd_650m = (\n",
    "    f'cd {REPO_DIR} && claude -p \"{prompt_650m}\" '\n",
    "    f'--allowedTools \"mcp__esm_mcp__extract_protein_embeddings,mcp__esm_mcp__esm_train_fitness_model,Bash,Read,Write\"'\n",
    ")\n",
    "os.system(cmd_650m)\n",
    "\n",
    "# --- 4.2: ESM2-3B (extract embeddings + train svr/xgboost/knn) ---\n",
    "prompt_3b = f\"\"\"\\\n",
    "Can you help train ESM models for data in {RESULTS_DIR}/ and save them to \\\n",
    "{RESULTS_DIR}/esm2_3B_{{head_model}} using the esm mcp server with svr, xgboost, \\\n",
    "and knn as the head models and esm2_t36_3B_UR50D as the backbone.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "Obtain the embeddings if they are not created.\n",
    "\"\"\"\n",
    "\n",
    "cmd_3b = (\n",
    "    f'cd {REPO_DIR} && claude -p \"{prompt_3b}\" '\n",
    "    f'--allowedTools \"mcp__esm_mcp__extract_protein_embeddings,mcp__esm_mcp__esm_train_fitness_model,Bash,Read,Write\"'\n",
    ")\n",
    "os.system(cmd_3b)\n",
    "\n",
    "# Verify\n",
    "for backbone in [\"esm2_650M\", \"esm2_3B\"]:\n",
    "    for head in [\"svr\", \"xgboost\", \"knn\"]:\n",
    "        d = f\"{RESULTS_DIR}/{backbone}_{head}\"\n",
    "        if os.path.isdir(d):\n",
    "            print(f\"  {backbone}_{head}: {os.listdir(d)}\")\n",
    "        else:\n",
    "            print(f\"  {backbone}_{head}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step5_prottrans",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Step 5 ‚Äî Build ProtTrans Models\n",
    "%%time\n",
    "import os\n",
    "\n",
    "REPO_DIR = \"/content/ProteinMCP\"\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "Can you help train ProtTrans models for data in {RESULTS_DIR}/ and save them to \\\n",
    "{RESULTS_DIR}/{{backbone_model}}_{{head_model}} using the prottrans mcp server with \\\n",
    "ProtT5-XL and ProtAlbert as backbone_models and knn, xgboost, and svr as the head models.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "Create the embeddings if they are not created.\n",
    "\"\"\"\n",
    "\n",
    "cmd = (\n",
    "    f'cd {REPO_DIR} && claude -p \"{prompt}\" '\n",
    "    f'--allowedTools \"mcp__prottrans_mcp__prottrans_extract_embeddings,mcp__prottrans_mcp__prottrans_train_fitness_model,Bash,Read,Write\"'\n",
    ")\n",
    "os.system(cmd)\n",
    "\n",
    "# Verify\n",
    "for backbone in [\"ProtT5-XL\", \"ProtAlbert\"]:\n",
    "    for head in [\"svr\", \"xgboost\", \"knn\"]:\n",
    "        d = f\"{RESULTS_DIR}/{backbone}_{head}\"\n",
    "        if os.path.isdir(d):\n",
    "            print(f\"  {backbone}_{head}: {os.listdir(d)}\")\n",
    "        else:\n",
    "            print(f\"  {backbone}_{head}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step6_visualize",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Step 6 ‚Äî Aggregate Results & Visualize\n",
    "%%time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "REPO_DIR = \"/content/ProteinMCP\"\n",
    "\n",
    "# ---- 6.1 Collect and aggregate all model results ----\n",
    "results = []\n",
    "\n",
    "# EV+OneHot ‚Äî metrics_summary.csv (stage/fold format)\n",
    "ev_path = os.path.join(RESULTS_DIR, \"metrics_summary.csv\")\n",
    "if os.path.exists(ev_path):\n",
    "    ev = pd.read_csv(ev_path)\n",
    "    cv_mean = ev[ev[\"fold\"] == \"mean\"][\"spearman_correlation\"].values[0]\n",
    "    cv_std  = ev[ev[\"fold\"] == \"std\"][\"spearman_correlation\"].values[0]\n",
    "    results.append({\"backbone\": \"EV+OneHot\", \"head\": \"ridge\",\n",
    "                    \"mean_cv_spearman\": cv_mean, \"std_cv_spearman\": cv_std})\n",
    "\n",
    "# ESM & ProtTrans ‚Äî training_summary.csv in subdirectories\n",
    "for dir_name in sorted(os.listdir(RESULTS_DIR)):\n",
    "    summary = os.path.join(RESULTS_DIR, dir_name, \"training_summary.csv\")\n",
    "    if not os.path.exists(summary):\n",
    "        continue\n",
    "    df = pd.read_csv(summary)\n",
    "    if \"mean_cv_spearman\" in df.columns:\n",
    "        mean_sp = df[\"mean_cv_spearman\"].values[0]\n",
    "        std_sp  = df[\"std_cv_spearman\"].values[0]\n",
    "    elif \"cv_mean\" in df.columns:\n",
    "        mean_sp = df[\"cv_mean\"].values[0]\n",
    "        std_sp  = df[\"cv_std\"].values[0]\n",
    "    else:\n",
    "        continue\n",
    "    parts = dir_name.rsplit(\"_\", 1)\n",
    "    if len(parts) == 2:\n",
    "        results.append({\"backbone\": parts[0], \"head\": parts[1],\n",
    "                        \"mean_cv_spearman\": mean_sp, \"std_cv_spearman\": std_sp})\n",
    "\n",
    "all_models = pd.DataFrame(results)\n",
    "all_models.to_csv(os.path.join(RESULTS_DIR, \"all_models_comparison.csv\"), index=False)\n",
    "print(f\"Saved {len(results)} model results to all_models_comparison.csv\\n\")\n",
    "print(all_models.sort_values(\"mean_cv_spearman\", ascending=False).to_string(index=False))\n",
    "\n",
    "# ---- 6.2 Generate four-panel visualization ----\n",
    "VIZ_SCRIPT = f\"{REPO_DIR}/workflow-skills/scripts/fitness_modeling_viz.py\"\n",
    "VIZ_PYTHON = f\"{REPO_DIR}/tool-mcps/ev_onehot_mcp/env/bin/python\"\n",
    "\n",
    "# Use ev_onehot_mcp env (has matplotlib, seaborn, scipy, Pillow)\n",
    "if os.path.exists(VIZ_PYTHON):\n",
    "    os.system(f\"{VIZ_PYTHON} {VIZ_SCRIPT} {RESULTS_DIR}\")\n",
    "else:\n",
    "    # Fallback: install deps in base env\n",
    "    os.system(f\"pip install -q matplotlib seaborn scipy Pillow\")\n",
    "    os.system(f\"python {VIZ_SCRIPT} {RESULTS_DIR}\")\n",
    "\n",
    "# ---- 6.3 Display figure inline ----\n",
    "from IPython.display import display, Image\n",
    "\n",
    "summary_png = os.path.join(RESULTS_DIR, \"figures\", \"fitness_modeling_summary.png\")\n",
    "if os.path.exists(summary_png):\n",
    "    print(\"\\nFour-panel summary:\")\n",
    "    display(Image(filename=summary_png, width=800))\n",
    "else:\n",
    "    # Try individual figures\n",
    "    figs_dir = os.path.join(RESULTS_DIR, \"figures\")\n",
    "    if os.path.isdir(figs_dir):\n",
    "        for f in sorted(os.listdir(figs_dir)):\n",
    "            if f.endswith(\".png\"):\n",
    "                print(f\"\\n{f}:\")\n",
    "                display(Image(filename=os.path.join(figs_dir, f), width=500))\n",
    "    else:\n",
    "        print(\"No figures generated ‚Äî check logs above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step7_download",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Step 7 ‚Äî Download Results\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Summary table ----\n",
    "csv_path = os.path.join(RESULTS_DIR, \"all_models_comparison.csv\")\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path).sort_values(\"mean_cv_spearman\", ascending=False)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL PERFORMANCE SUMMARY (5-fold CV Spearman œÅ)\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, row in df.iterrows():\n",
    "        print(f\"  {row['backbone']:15} + {row['head']:8}: \"\n",
    "              f\"{row['mean_cv_spearman']:.3f} ¬± {row['std_cv_spearman']:.3f}\")\n",
    "    print(\"=\" * 60)\n",
    "    best = df.iloc[0]\n",
    "    print(f\"  Best: {best['backbone']} ({best['head']}) ‚Äî œÅ = {best['mean_cv_spearman']:.3f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# ---- Zip and download ----\n",
    "zip_path = f\"/content/{PROTEIN_NAME}_results.zip\"\n",
    "os.system(f'cd /content && zip -r \"{zip_path}\" \"results/{PROTEIN_NAME}\"')\n",
    "print(f\"\\nResults zipped to {zip_path}\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üßπ Cleanup (optional)\n",
    "#@markdown Run this cell to uninstall the fitness modeling skill and remove temp files.\n",
    "\n",
    "import os\n",
    "\n",
    "REPO_DIR = \"/content/ProteinMCP\"\n",
    "\n",
    "# Uninstall skill and MCPs\n",
    "os.system(f\"cd {REPO_DIR} && pskill uninstall fitness_modeling\")\n",
    "\n",
    "# Remove flag files\n",
    "for flag in [\"/content/.conda_ready\", \"/content/.proteinmcp_ready\", \"/content/.skill_ready\"]:\n",
    "    if os.path.exists(flag):\n",
    "        os.remove(flag)\n",
    "\n",
    "print(\"Cleanup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "---\n",
    "## Instructions & Troubleshooting\n",
    "\n",
    "### Data Format\n",
    "Your `data.csv` must contain at minimum:\n",
    "- **`seq`** ‚Äî Full protein sequence\n",
    "- **`log_fitness`** ‚Äî Log-transformed fitness value (target)\n",
    "\n",
    "Your `wt.fasta` should contain the wild-type reference sequence in standard FASTA format.\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| `uniref100.model_params not found` | Re-run Step 2 ‚Äî symlinks may not have been created |\n",
    "| `wt.fasta not found` in EV+OneHot | Ensure wt.fasta is in RESULTS_DIR (Step 0) |\n",
    "| ESM embeddings extraction fails | The `claude -p` call will fall back to `esm-extract` CLI |\n",
    "| GPU Out of Memory | Use **Runtime ‚Üí Change runtime type ‚Üí T4**; or skip ESM2-3B |\n",
    "| Low Spearman correlation | Check data quality; ensure proper log-transformation |\n",
    "| MCP not found | Re-run the skill install cell |\n",
    "\n",
    "### Model Performance Reference\n",
    "\n",
    "| Model | Typical CV Spearman | Best Use |\n",
    "|-------|-------------------|----------|\n",
    "| EV+OneHot | 0.20‚Äì0.35 | Baseline, interpretable |\n",
    "| ESM2-650M | 0.15‚Äì0.25 | Fast, good balance |\n",
    "| ESM2-3B | 0.18‚Äì0.28 | Higher accuracy |\n",
    "| ProtT5-XL | 0.15‚Äì0.25 | Alternative to ESM |\n",
    "| ProtAlbert | 0.08‚Äì0.15 | Lightweight option |\n",
    "\n",
    "**Recommended head models:** SVR (most stable), XGBoost (higher potential), KNN (simple baseline)\n",
    "\n",
    "### References\n",
    "- [ESM](https://github.com/facebookresearch/esm) ‚Äî Meta's protein language models\n",
    "- [ProtTrans](https://github.com/agemagician/ProtTrans) ‚Äî Protein transformer embeddings\n",
    "- [PLMC](https://github.com/debbiemarkslab/plmc) ‚Äî Evolutionary coupling analysis\n",
    "- [ProteinMCP](https://github.com/charlesxu90/ProteinMCP) ‚Äî This project"
   ]
  }
 ]
}
