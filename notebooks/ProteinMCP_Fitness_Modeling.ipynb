{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title_header"
   },
   "source": [
    "# ProteinMCP ‚Äî Fitness Modeling Workflow\n",
    "\n",
    "Build and compare protein fitness prediction models using multiple backbone architectures:\n",
    "\n",
    "| Model | Description |\n",
    "|-------|-------------|\n",
    "| **EV+OneHot** | Evolutionary couplings + one-hot encoding (PLMC) |\n",
    "| **ESM2-650M / ESM2-3B** | Meta's protein language models |\n",
    "| **ProtT5-XL / ProtAlbert** | ProtTrans transformer embeddings |\n",
    "\n",
    "Each model is trained with SVR, XGBoost, and KNN heads, then compared via 5-fold cross-validated Spearman correlation.\n",
    "\n",
    "**Prerequisites:** Docker, Claude Code CLI, ProteinMCP installed locally.\n",
    "\n",
    "**Links:** [GitHub](https://github.com/charlesxu90/ProteinMCP) ¬∑ [ESM](https://github.com/facebookresearch/esm) ¬∑ [ProtTrans](https://github.com/agemagician/ProtTrans) ¬∑ [PLMC](https://github.com/debbiemarkslab/plmc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ User Configuration ‚îÄ‚îÄ\n",
    "PROTEIN_NAME = \"TEVp_S219V\"\n",
    "use_example_data = True\n",
    "\n",
    "ANTHROPIC_API_KEY = \"\"  # Get one at https://console.anthropic.com/\n",
    "CLAUDE_MODEL = \"claude-haiku-4-5-20251001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import utility and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "user_config"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import select\n",
    "\n",
    "# ---------- Streaming command runner ----------\n",
    "def run_cmd(cmd, cwd=None):\n",
    "    \"\"\"Run a shell command and stream stdout/stderr line-by-line in real time.\"\"\"\n",
    "    proc = subprocess.Popen(\n",
    "        cmd, shell=True, cwd=cwd,\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        bufsize=1, text=True,\n",
    "    )\n",
    "    for line in proc.stdout:\n",
    "        print(line, end=\"\", flush=True)\n",
    "    proc.wait()\n",
    "    if proc.returncode != 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Command exited with code {proc.returncode}\")\n",
    "    return proc.returncode\n",
    "\n",
    "# ---------- Claude streaming helper ----------\n",
    "def _display_claude_line(line):\n",
    "    \"\"\"Parse a single stream-json line from Claude CLI and print progress.\"\"\"\n",
    "    if not line.strip():\n",
    "        return\n",
    "    try:\n",
    "        data = json.loads(line)\n",
    "        msg_type = data.get('type', '')\n",
    "        subtype = data.get('subtype', '')\n",
    "\n",
    "        if msg_type == 'system':\n",
    "            if subtype == 'init':\n",
    "                session_id = data.get('session_id', '')[:8]\n",
    "                print(f\"  ü§ñ Session started: {session_id}...\", flush=True)\n",
    "            elif subtype != 'transcript':\n",
    "                print(f\"  ‚öôÔ∏è  System: {subtype}\", flush=True)\n",
    "\n",
    "        elif msg_type == 'assistant':\n",
    "            message = data.get('message', {})\n",
    "            for block in message.get('content', []):\n",
    "                block_type = block.get('type', '')\n",
    "                if block_type == 'thinking':\n",
    "                    text = block.get('thinking', '')[:100]\n",
    "                    print(f\"  üí≠ Thinking: {text}...\", flush=True)\n",
    "                elif block_type == 'text':\n",
    "                    lines = block.get('text', '').strip().split('\\n')\n",
    "                    for tl in lines[:5]:\n",
    "                        if tl.strip():\n",
    "                            print(f\"  {tl}\", flush=True)\n",
    "                    if len(lines) > 5:\n",
    "                        print(f\"  ... ({len(lines) - 5} more lines)\", flush=True)\n",
    "                elif block_type == 'tool_use':\n",
    "                    tool_name = block.get('name', 'unknown')\n",
    "                    tool_input = block.get('input', {})\n",
    "                    if tool_name == 'Bash':\n",
    "                        print(f\"  üîß Bash: {tool_input.get('command', '')[:80]}\", flush=True)\n",
    "                    elif tool_name in ('Read', 'Write', 'Edit'):\n",
    "                        print(f\"  üìñ {tool_name}: {tool_input.get('file_path', '')}\", flush=True)\n",
    "                    elif tool_name.startswith('mcp__'):\n",
    "                        print(f\"  üîå MCP: {tool_name}\", flush=True)\n",
    "                    else:\n",
    "                        print(f\"  üîß {tool_name}\", flush=True)\n",
    "\n",
    "        elif msg_type == 'user':\n",
    "            for block in data.get('message', {}).get('content', []):\n",
    "                if block.get('type') == 'tool_result':\n",
    "                    if block.get('is_error', False):\n",
    "                        err = block.get('content', '')\n",
    "                        err = err[:100] if isinstance(err, str) else str(err)[:100]\n",
    "                        print(f\"  ‚ùå Error: {err}\", flush=True)\n",
    "                    else:\n",
    "                        content = block.get('content', '')\n",
    "                        if isinstance(content, str) and content.strip():\n",
    "                            first = content.strip().split('\\n')[0][:80]\n",
    "                            if first:\n",
    "                                print(f\"  ‚úÖ Result: {first}\", flush=True)\n",
    "                        else:\n",
    "                            print(f\"  ‚úÖ Done\", flush=True)\n",
    "\n",
    "        elif msg_type == 'result':\n",
    "            if subtype == 'success':\n",
    "                print(f\"  ‚úÖ Completed successfully\", flush=True)\n",
    "            elif subtype == 'error':\n",
    "                print(f\"  ‚ùå Error: {data.get('error', 'Unknown')}\", flush=True)\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        if line.strip():\n",
    "            print(f\"  {line}\", flush=True)\n",
    "\n",
    "\n",
    "def run_claude(prompt, allowed_tools=None, cwd=None):\n",
    "    \"\"\"Run Claude CLI with real-time streaming output.\n",
    "\n",
    "    Args:\n",
    "        prompt: The prompt text to send to Claude (passed via stdin).\n",
    "        allowed_tools: Comma-separated tool names, e.g. \"Bash,Read,Write\".\n",
    "        cwd: Working directory for the claude process.\n",
    "\n",
    "    Returns:\n",
    "        Process return code (0 = success).\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        \"claude\",\n",
    "        \"--model\", CLAUDE_MODEL,\n",
    "        \"-p\", \"-\",\n",
    "        \"--output-format\", \"stream-json\",\n",
    "        \"--verbose\",\n",
    "        \"--dangerously-skip-permissions\",\n",
    "    ]\n",
    "    if allowed_tools:\n",
    "        cmd += [\"--allowedTools\", allowed_tools]\n",
    "\n",
    "    print(f\"  ü§ñ Claude model: {CLAUDE_MODEL}\")\n",
    "    print(f\"  üìã Tools: {allowed_tools or 'all'}\")\n",
    "    print(f\"  \" + \"-\" * 58)\n",
    "\n",
    "    proc = subprocess.Popen(\n",
    "        cmd, cwd=cwd,\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True, bufsize=1,\n",
    "    )\n",
    "    proc.stdin.write(prompt)\n",
    "    proc.stdin.close()\n",
    "\n",
    "    while True:\n",
    "        if proc.poll() is not None:\n",
    "            # Drain remaining output\n",
    "            for line in (proc.stdout.read() or '').split('\\n'):\n",
    "                _display_claude_line(line)\n",
    "            for line in (proc.stderr.read() or '').split('\\n'):\n",
    "                if line.strip():\n",
    "                    print(f\"  ‚öôÔ∏è  {line}\", flush=True)\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            readable, _, _ = select.select([proc.stdout, proc.stderr], [], [], 0.1)\n",
    "        except (ValueError, OSError):\n",
    "            break\n",
    "\n",
    "        for stream in readable:\n",
    "            line = stream.readline()\n",
    "            if line:\n",
    "                if stream == proc.stdout:\n",
    "                    _display_claude_line(line.rstrip('\\n'))\n",
    "                else:\n",
    "                    if line.strip():\n",
    "                        print(f\"  ‚öôÔ∏è  {line.rstrip()}\", flush=True)\n",
    "\n",
    "    rc = proc.wait()\n",
    "    print(f\"  \" + \"-\" * 58)\n",
    "    if rc != 0:\n",
    "        print(f\"  ‚ö†Ô∏è  Claude exited with code {rc}\")\n",
    "    return rc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and verify the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use bundled example data for TEVp_S219V\n",
      "\n",
      "CLAUDE_MODEL : claude-haiku-4-5-20251001\n",
      "PROTEIN_NAME : TEVp_S219V\n",
      "REPO_DIR     : /home/xux/Desktop/AgentMCP/ProteinMCP\n",
      "DATA_DIR     : /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V\n",
      "RESULTS_DIR  : /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V\n",
      "WT_FASTA     : /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/wt.fasta\n",
      "DATA_CSV     : /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/data.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- Validate API key ----------\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY is required. Get one at https://console.anthropic.com/\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "\n",
    "# ---------- Paths ----------\n",
    "# Auto-detect REPO_DIR: walk up from notebook location to find project root\n",
    "_nb_dir = os.path.abspath(\"\")\n",
    "if os.path.basename(_nb_dir) == \"notebooks\":\n",
    "    REPO_DIR = os.path.dirname(_nb_dir)\n",
    "else:\n",
    "    REPO_DIR = _nb_dir\n",
    "\n",
    "DATA_DIR    = os.path.join(REPO_DIR, \"data\", PROTEIN_NAME)\n",
    "RESULTS_DIR = os.path.join(REPO_DIR, \"results\", PROTEIN_NAME)\n",
    "\n",
    "WT_FASTA = os.path.join(DATA_DIR, \"wt.fasta\")\n",
    "DATA_CSV = os.path.join(DATA_DIR, \"data.csv\")\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "if use_example_data:\n",
    "    print(f\"Will use bundled example data for {PROTEIN_NAME}\")\n",
    "else:\n",
    "    print(f\"Please place your wt.fasta and data.csv files in:\\n  {DATA_DIR}\")\n",
    "    input(\"Press Enter once the files are in place...\")\n",
    "    assert os.path.exists(WT_FASTA), f\"Missing {WT_FASTA} ‚Äî please provide wt.fasta\"\n",
    "    assert os.path.exists(DATA_CSV),  f\"Missing {DATA_CSV} ‚Äî please provide data.csv\"\n",
    "\n",
    "print(f\"\\nCLAUDE_MODEL : {CLAUDE_MODEL}\")\n",
    "print(f\"PROTEIN_NAME : {PROTEIN_NAME}\")\n",
    "print(f\"REPO_DIR     : {REPO_DIR}\")\n",
    "print(f\"DATA_DIR     : {DATA_DIR}\")\n",
    "print(f\"RESULTS_DIR  : {RESULTS_DIR}\")\n",
    "print(f\"WT_FASTA     : {WT_FASTA}\")\n",
    "print(f\"DATA_CSV     : {DATA_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Install Fitness Modeling Skill & MCPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xux/miniforge3/envs/protein-mcp/bin/pmcp\n",
      "ProteinMCP already installed.\n",
      "/home/xux/.local/bin/claude\n",
      "Claude Code already installed.\n",
      "Docker version 29.2.1, build a5c7197\n",
      "Docker found.\n",
      "ProteinMCP & Claude Code ready.\n",
      "ANTHROPIC_API_KEY set (ends with ...CQAA)\n",
      "\n",
      "Elapsed: 0.0s\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "# Install ProteinMCP if not already present\n",
    "if run_cmd(\"which pmcp\") != 0:\n",
    "    run_cmd(f\"pip install -e {REPO_DIR}\")\n",
    "    run_cmd(f\"pip install -r {REPO_DIR}/requirements.txt\")\n",
    "else:\n",
    "    print(\"ProteinMCP already installed.\")\n",
    "\n",
    "# Install Claude Code if not already present\n",
    "if run_cmd(\"which claude\") != 0:\n",
    "    run_cmd(\"npm install -g @anthropic-ai/claude-code\")\n",
    "else:\n",
    "    print(\"Claude Code already installed.\")\n",
    "\n",
    "# Verify Docker is available (required for Docker MCPs)\n",
    "if run_cmd(\"docker --version\") != 0:\n",
    "    print(\"WARNING: Docker not found. Docker MCPs will not work.\")\n",
    "else:\n",
    "    print(\"Docker found.\")\n",
    "\n",
    "print(\"ProteinMCP & Claude Code ready.\")\n",
    "\n",
    "# Set API key for Claude Code\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "print(f\"ANTHROPIC_API_KEY set (ends with ...{ANTHROPIC_API_KEY[-4:]})\")\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing skill 'fitness_modeling'...\n",
      "  Copied skill to: /home/xux/Desktop/AgentMCP/ProteinMCP/.claude/skills/fitness-modeling.md\n",
      "  Created command: /home/xux/Desktop/AgentMCP/ProteinMCP/.claude/commands/fitness-model.md\n",
      "\n",
      "üìä Checking status of 5 required MCPs...\n",
      "\n",
      "‚úÖ Already installed (3):\n",
      "    ‚Ä¢ plmc_mcp\n",
      "    ‚Ä¢ ev_onehot_mcp\n",
      "    ‚Ä¢ prottrans_mcp\n",
      "\n",
      "‚ö° Installing 2 MCPs in parallel...\n",
      "‚úÖ MCP 'msa_mcp' is already installed at: tool-mcps/msa_mcp\n",
      "  ‚úÖ msa_mcp: OK\n",
      "üê≥ Pulling Docker image: ghcr.io/macromnex/esm_mcp:latest\n",
      "latest: Pulling from macromnex/esm_mcp\n",
      "9b857f539cb1: Already exists\n",
      "0f646cd818b2: Already exists\n",
      "11b9eb8c1e25: Already exists\n",
      "4f4fb700ef54: Already exists\n",
      "9f1d89b66463: Already exists\n",
      "bd64244c5e36: Pulling fs layer\n",
      "31aeef00868d: Pulling fs layer\n",
      "2a493ea96ce6: Pulling fs layer\n",
      "d537a5b76f40: Pulling fs layer\n",
      "6aad6996ec02: Pulling fs layer\n",
      "caaf0c62f6a6: Pulling fs layer\n",
      "7256639a4991: Pulling fs layer\n",
      "0c9e3001b0e8: Pulling fs layer\n",
      "e70ae3d6a00b: Pulling fs layer\n",
      "10f4e1bfc932: Pulling fs layer\n",
      "190f36c77a82: Pulling fs layer\n",
      "236a9fb8eb5d: Pulling fs layer\n",
      "e9a1c1b0ee80: Pulling fs layer\n",
      "7256639a4991: Waiting\n",
      "0c9e3001b0e8: Waiting\n",
      "e70ae3d6a00b: Waiting\n",
      "10f4e1bfc932: Waiting\n",
      "190f36c77a82: Waiting\n",
      "236a9fb8eb5d: Waiting\n",
      "e9a1c1b0ee80: Waiting\n",
      "d537a5b76f40: Waiting\n",
      "6aad6996ec02: Waiting\n",
      "caaf0c62f6a6: Waiting\n",
      "31aeef00868d: Download complete\n",
      "2a493ea96ce6: Verifying Checksum\n",
      "2a493ea96ce6: Download complete\n",
      "6aad6996ec02: Verifying Checksum\n",
      "6aad6996ec02: Download complete\n",
      "bd64244c5e36: Verifying Checksum\n",
      "bd64244c5e36: Download complete\n",
      "bd64244c5e36: Pull complete\n",
      "31aeef00868d: Pull complete\n",
      "2a493ea96ce6: Pull complete\n",
      "7256639a4991: Verifying Checksum\n",
      "7256639a4991: Download complete\n",
      "caaf0c62f6a6: Verifying Checksum\n",
      "caaf0c62f6a6: Download complete\n",
      "0c9e3001b0e8: Verifying Checksum\n",
      "0c9e3001b0e8: Download complete\n",
      "e70ae3d6a00b: Verifying Checksum\n",
      "e70ae3d6a00b: Download complete\n",
      "190f36c77a82: Download complete\n",
      "236a9fb8eb5d: Verifying Checksum\n",
      "236a9fb8eb5d: Download complete\n",
      "e9a1c1b0ee80: Download complete\n",
      "d537a5b76f40: Verifying Checksum\n",
      "d537a5b76f40: Download complete\n",
      "d537a5b76f40: Pull complete\n",
      "6aad6996ec02: Pull complete\n",
      "caaf0c62f6a6: Pull complete\n",
      "7256639a4991: Pull complete\n",
      "0c9e3001b0e8: Pull complete\n",
      "e70ae3d6a00b: Pull complete\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "# Install the fitness_modeling skill (pulls Docker images for msa_mcp, plmc_mcp, ev_onehot_mcp, esm_mcp, prottrans_mcp)\n",
    "run_cmd(\"pskill install fitness_modeling\", cwd=REPO_DIR)\n",
    "print(\"Fitness modeling skill installed.\")\n",
    "\n",
    "# Verify MCP status\n",
    "run_cmd(\"pmcp status\", cwd=REPO_DIR)\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 ‚Äî Setup Results Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "EXAMPLE_DIR = os.path.join(REPO_DIR, \"examples\", \"case1_fitness_modeling\")\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "if use_example_data:\n",
    "    # Copy bundled example data into DATA_DIR\n",
    "    for fname in [\"wt.fasta\", \"data.csv\"]:\n",
    "        src = os.path.join(EXAMPLE_DIR, fname)\n",
    "        dst = os.path.join(DATA_DIR, fname)\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copy2(src, dst)\n",
    "            print(f\"Copied {src} ‚Üí {dst}\")\n",
    "\n",
    "# Copy input files to RESULTS_DIR (needed by training tools)\n",
    "for fname in [\"wt.fasta\", \"data.csv\"]:\n",
    "    src = os.path.join(DATA_DIR, fname)\n",
    "    dst = os.path.join(RESULTS_DIR, fname)\n",
    "    if not os.path.exists(dst):\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"Copied {src} ‚Üí {dst}\")\n",
    "\n",
    "# Verify\n",
    "assert os.path.exists(os.path.join(RESULTS_DIR, \"wt.fasta\")), \"wt.fasta missing in RESULTS_DIR\"\n",
    "assert os.path.exists(os.path.join(RESULTS_DIR, \"data.csv\")),  \"data.csv missing in RESULTS_DIR\"\n",
    "print(f\"\\nResults directory ready: {RESULTS_DIR}\")\n",
    "print(f\"Files: {os.listdir(RESULTS_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 ‚Äî Generate MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "step1_msa"
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "Can you obtain the MSA for {PROTEIN_NAME} from {WT_FASTA} using msa mcp \\\n",
    "and save it to {RESULTS_DIR}/{PROTEIN_NAME}.a3m.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt,\n",
    "    allowed_tools=\"mcp__msa_mcp__generate_msa,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# Verify output\n",
    "msa_file = f\"{RESULTS_DIR}/{PROTEIN_NAME}.a3m\"\n",
    "assert os.path.exists(msa_file), f\"MSA file not found: {msa_file}\"\n",
    "print(f\"\\nMSA generated: {msa_file}\")\n",
    "print(f\"Elapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 ‚Äî Build PLMC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "step2_plmc"
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "I have created an a3m file in {RESULTS_DIR}/{PROTEIN_NAME}.a3m. \\\n",
    "Can you help build an EV model using plmc mcp and save it to {RESULTS_DIR}/plmc directory. \\\n",
    "The wild-type sequence is {WT_FASTA}.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "After building the model, create symlinks in {RESULTS_DIR}/plmc/:\n",
    "  ln -sf {PROTEIN_NAME}.model_params uniref100.model_params\n",
    "  ln -sf {PROTEIN_NAME}.EC uniref100.EC\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt,\n",
    "    allowed_tools=\"mcp__plmc_mcp__plmc_convert_a3m_to_a2m,mcp__plmc_mcp__plmc_generate_model,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# Verify outputs\n",
    "plmc_dir = f\"{RESULTS_DIR}/plmc\"\n",
    "assert os.path.exists(f\"{plmc_dir}/uniref100.model_params\"), \"PLMC model_params symlink missing\"\n",
    "assert os.path.exists(f\"{plmc_dir}/uniref100.EC\"), \"PLMC EC symlink missing\"\n",
    "print(f\"\\nPLMC model built: {os.listdir(plmc_dir)}\")\n",
    "print(f\"Elapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äî Build EV+OneHot Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "step3_ev_onehot"
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "I have created a plmc model in directory {RESULTS_DIR}/plmc. \\\n",
    "Can you help build an EV+OneHot model using ev_onehot_mcp and save it to {RESULTS_DIR}/ directory. \\\n",
    "The wild-type sequence is {RESULTS_DIR}/wt.fasta, and the dataset is {RESULTS_DIR}/data.csv.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt,\n",
    "    allowed_tools=\"mcp__ev_onehot_mcp__ev_onehot_train_fitness_predictor,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# Verify output\n",
    "assert os.path.exists(f\"{RESULTS_DIR}/metrics_summary.csv\"), \"EV+OneHot metrics not found\"\n",
    "print(f\"\\nEV+OneHot model trained. Metrics:\")\n",
    "import pandas as pd\n",
    "print(pd.read_csv(f\"{RESULTS_DIR}/metrics_summary.csv\").to_string(index=False))\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äî Build ESM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "step4_esm"
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "# --- 4.1: ESM2-650M (extract embeddings + train svr/xgboost/knn) ---\n",
    "print(\"=\"*60)\n",
    "print(\"ESM2-650M: extracting embeddings & training models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt_650m = f\"\"\"\\\n",
    "Can you help train ESM models for data in {RESULTS_DIR}/ and save them to \\\n",
    "{RESULTS_DIR}/esm2_650M_{{head_model}} using the esm_mcp server with svr, xgboost, \\\n",
    "and knn as the head models.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "Obtain the embeddings if they are not created.\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt_650m,\n",
    "    allowed_tools=\"mcp__esm_mcp__extract_protein_embeddings,mcp__esm_mcp__esm_train_fitness_model,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# --- 4.2: ESM2-3B (extract embeddings + train svr/xgboost/knn) ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ESM2-3B: extracting embeddings & training models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt_3b = f\"\"\"\\\n",
    "Can you help train ESM models for data in {RESULTS_DIR}/ and save them to \\\n",
    "{RESULTS_DIR}/esm2_3B_{{head_model}} using the esm mcp server with svr, xgboost, \\\n",
    "and knn as the head models and esm2_t36_3B_UR50D as the backbone.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "Obtain the embeddings if they are not created.\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt_3b,\n",
    "    allowed_tools=\"mcp__esm_mcp__extract_protein_embeddings,mcp__esm_mcp__esm_train_fitness_model,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ESM model outputs:\")\n",
    "print(\"=\"*60)\n",
    "for backbone in [\"esm2_650M\", \"esm2_3B\"]:\n",
    "    for head in [\"svr\", \"xgboost\", \"knn\"]:\n",
    "        d = f\"{RESULTS_DIR}/{backbone}_{head}\"\n",
    "        if os.path.isdir(d):\n",
    "            print(f\"  ‚úì {backbone}_{head}: {os.listdir(d)}\")\n",
    "        else:\n",
    "            print(f\"  ‚úó {backbone}_{head}: NOT FOUND\")\n",
    "\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 ‚Äî Build ProtTrans Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "step5_prottrans"
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ProtTrans: extracting embeddings & training models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "Can you help train ProtTrans models for data in {RESULTS_DIR}/ and save them to \\\n",
    "{RESULTS_DIR}/{{backbone_model}}_{{head_model}} using the prottrans mcp server with \\\n",
    "ProtT5-XL and ProtAlbert as backbone_models and knn, xgboost, and svr as the head models.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "Create the embeddings if they are not created.\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt,\n",
    "    allowed_tools=\"mcp__prottrans_mcp__prottrans_extract_embeddings,mcp__prottrans_mcp__prottrans_train_fitness_model,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ProtTrans model outputs:\")\n",
    "print(\"=\"*60)\n",
    "for backbone in [\"ProtT5-XL\", \"ProtAlbert\"]:\n",
    "    for head in [\"svr\", \"xgboost\", \"knn\"]:\n",
    "        d = f\"{RESULTS_DIR}/{backbone}_{head}\"\n",
    "        if os.path.isdir(d):\n",
    "            print(f\"  ‚úì {backbone}_{head}: {os.listdir(d)}\")\n",
    "        else:\n",
    "            print(f\"  ‚úó {backbone}_{head}: NOT FOUND\")\n",
    "\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 ‚Äî Aggregate Results & Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "troubleshooting"
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "import pandas as pd\n",
    "_t0 = time.time()\n",
    "\n",
    "# ---- 6.1 Collect and aggregate all model results ----\n",
    "results = []\n",
    "\n",
    "# EV+OneHot ‚Äî metrics_summary.csv (stage/fold format)\n",
    "ev_path = os.path.join(RESULTS_DIR, \"metrics_summary.csv\")\n",
    "if os.path.exists(ev_path):\n",
    "    ev = pd.read_csv(ev_path)\n",
    "    cv_mean = ev[ev[\"fold\"] == \"mean\"][\"spearman_correlation\"].values[0]\n",
    "    cv_std  = ev[ev[\"fold\"] == \"std\"][\"spearman_correlation\"].values[0]\n",
    "    results.append({\"backbone\": \"EV+OneHot\", \"head\": \"ridge\",\n",
    "                    \"mean_cv_spearman\": cv_mean, \"std_cv_spearman\": cv_std})\n",
    "\n",
    "# ESM & ProtTrans ‚Äî training_summary.csv in subdirectories\n",
    "for dir_name in sorted(os.listdir(RESULTS_DIR)):\n",
    "    summary = os.path.join(RESULTS_DIR, dir_name, \"training_summary.csv\")\n",
    "    if not os.path.exists(summary):\n",
    "        continue\n",
    "    df = pd.read_csv(summary)\n",
    "    if \"mean_cv_spearman\" in df.columns:\n",
    "        mean_sp = df[\"mean_cv_spearman\"].values[0]\n",
    "        std_sp  = df[\"std_cv_spearman\"].values[0]\n",
    "    elif \"cv_mean\" in df.columns:\n",
    "        mean_sp = df[\"cv_mean\"].values[0]\n",
    "        std_sp  = df[\"cv_std\"].values[0]\n",
    "    else:\n",
    "        continue\n",
    "    parts = dir_name.rsplit(\"_\", 1)\n",
    "    if len(parts) == 2:\n",
    "        results.append({\"backbone\": parts[0], \"head\": parts[1],\n",
    "                        \"mean_cv_spearman\": mean_sp, \"std_cv_spearman\": std_sp})\n",
    "\n",
    "all_models = pd.DataFrame(results)\n",
    "all_models.to_csv(os.path.join(RESULTS_DIR, \"all_models_comparison.csv\"), index=False)\n",
    "print(f\"Saved {len(results)} model results to all_models_comparison.csv\\n\")\n",
    "print(all_models.sort_values(\"mean_cv_spearman\", ascending=False).to_string(index=False))\n",
    "\n",
    "# ---- 6.2 Generate four-panel visualization ----\n",
    "VIZ_SCRIPT = os.path.join(REPO_DIR, \"workflow-skills\", \"scripts\", \"fitness_modeling_viz.py\")\n",
    "\n",
    "# Install viz deps if needed, then run with system python\n",
    "run_cmd(\"pip install -q matplotlib seaborn scipy Pillow\")\n",
    "run_cmd(f\"python {VIZ_SCRIPT} {RESULTS_DIR}\")\n",
    "\n",
    "# ---- 6.3 Display figure inline ----\n",
    "from IPython.display import display, Image\n",
    "\n",
    "summary_png = os.path.join(RESULTS_DIR, \"figures\", \"fitness_modeling_summary.png\")\n",
    "if os.path.exists(summary_png):\n",
    "    print(\"\\nFour-panel summary:\")\n",
    "    display(Image(filename=summary_png, width=800))\n",
    "else:\n",
    "    # Try individual figures\n",
    "    figs_dir = os.path.join(RESULTS_DIR, \"figures\")\n",
    "    if os.path.isdir(figs_dir):\n",
    "        for f in sorted(os.listdir(figs_dir)):\n",
    "            if f.endswith(\".png\"):\n",
    "                print(f\"\\n{f}:\")\n",
    "                display(Image(filename=os.path.join(figs_dir, f), width=500))\n",
    "    else:\n",
    "        print(\"No figures generated ‚Äî check logs above.\")\n",
    "\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 ‚Äî Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Summary table ----\n",
    "csv_path = os.path.join(RESULTS_DIR, \"all_models_comparison.csv\")\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path).sort_values(\"mean_cv_spearman\", ascending=False)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL PERFORMANCE SUMMARY (5-fold CV Spearman \\u03c1)\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, row in df.iterrows():\n",
    "        print(f\"  {row['backbone']:15} + {row['head']:8}: \"\n",
    "              f\"{row['mean_cv_spearman']:.3f} \\u00b1 {row['std_cv_spearman']:.3f}\")\n",
    "    print(\"=\" * 60)\n",
    "    best = df.iloc[0]\n",
    "    print(f\"  Best: {best['backbone']} ({best['head']}) \\u2014 \\u03c1 = {best['mean_cv_spearman']:.3f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# ---- Zip results ----\n",
    "zip_path = os.path.join(REPO_DIR, f\"{PROTEIN_NAME}_results.zip\")\n",
    "run_cmd(f'cd \"{os.path.dirname(RESULTS_DIR)}\" && zip -r \"{zip_path}\" \"{PROTEIN_NAME}\"')\n",
    "print(f\"\\nResults available at: {RESULTS_DIR}\")\n",
    "print(f\"Zipped archive: {zip_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Instructions & Troubleshooting\n",
    "\n",
    "### Data Format\n",
    "Your `data.csv` must contain at minimum:\n",
    "- **`seq`** ‚Äî Full protein sequence\n",
    "- **`log_fitness`** ‚Äî Log-transformed fitness value (target)\n",
    "\n",
    "Your `wt.fasta` should contain the wild-type reference sequence in standard FASTA format.\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| `uniref100.model_params not found` | Re-run Step 2 ‚Äî symlinks may not have been created |\n",
    "| `wt.fasta not found` in EV+OneHot | Ensure wt.fasta is in RESULTS_DIR (Step 0) |\n",
    "| ESM embeddings extraction fails | Check that `esm_mcp` Docker image is pulled (`pmcp status`) |\n",
    "| GPU Out of Memory | Try smaller backbone (ESM2-650M instead of 3B) |\n",
    "| Low Spearman correlation | Check data quality; ensure proper log-transformation |\n",
    "| MCP not found | Re-run `pskill install fitness_modeling` |\n",
    "| Docker image not pulled | Run `pmcp install <mcp_name>` to pull the image |\n",
    "\n",
    "### Model Performance Reference\n",
    "\n",
    "| Model | Typical CV Spearman | Best Use |\n",
    "|-------|-------------------|----------|\n",
    "| EV+OneHot | 0.20‚Äì0.35 | Baseline, interpretable |\n",
    "| ESM2-650M | 0.15‚Äì0.25 | Fast, good balance |\n",
    "| ESM2-3B | 0.18‚Äì0.28 | Higher accuracy |\n",
    "| ProtT5-XL | 0.15‚Äì0.25 | Alternative to ESM |\n",
    "| ProtAlbert | 0.08‚Äì0.15 | Lightweight option |\n",
    "\n",
    "**Recommended head models:** SVR (most stable), XGBoost (higher potential), KNN (simple baseline)\n",
    "\n",
    "### References\n",
    "- [ESM](https://github.com/facebookresearch/esm) ‚Äî Meta's protein language models\n",
    "- [ProtTrans](https://github.com/agemagician/ProtTrans) ‚Äî Protein transformer embeddings\n",
    "- [PLMC](https://github.com/debbiemarkslab/plmc) ‚Äî Evolutionary coupling analysis\n",
    "- [ProteinMCP](https://github.com/charlesxu90/ProteinMCP) ‚Äî This project"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "protein-mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}