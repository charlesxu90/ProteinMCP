{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title_header"
   },
   "source": [
    "# ProteinMCP ‚Äî Fitness Modeling Workflow\n",
    "\n",
    "Build and compare protein fitness prediction models using multiple backbone architectures:\n",
    "\n",
    "| Model | Description |\n",
    "|-------|-------------|\n",
    "| **EV+OneHot** | Evolutionary couplings + one-hot encoding (PLMC) |\n",
    "| **ESM2-650M / ESM2-3B** | Meta's protein language models |\n",
    "| **ProtT5-XL / ProtAlbert** | ProtTrans transformer embeddings |\n",
    "\n",
    "Each model is trained with SVR, XGBoost, and KNN heads, then compared via 5-fold cross-validated Spearman correlation.\n",
    "\n",
    "**Prerequisites:** Docker, Claude Code CLI, ProteinMCP installed locally.\n",
    "\n",
    "**Links:** [GitHub](https://github.com/charlesxu90/ProteinMCP) ¬∑ [ESM](https://github.com/facebookresearch/esm) ¬∑ [ProtTrans](https://github.com/agemagician/ProtTrans) ¬∑ [PLMC](https://github.com/debbiemarkslab/plmc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ‚îÄ‚îÄ User Configuration ‚îÄ‚îÄ\nPROTEIN_NAME = \"TEVp_S219V\"\nuse_example_data = True\n\n# Optional: set API key here or in .env. If unset, Claude CLI uses your logged-in account.\nANTHROPIC_API_KEY = \"\"\nCLAUDE_MODEL = \"claude-sonnet-4-6\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import utility and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "user_config"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import select\n",
    "\n",
    "# ---------- Streaming command runner ----------\n",
    "def run_cmd(cmd, cwd=None):\n",
    "    \"\"\"Run a shell command and stream stdout/stderr line-by-line in real time.\"\"\"\n",
    "    proc = subprocess.Popen(\n",
    "        cmd, shell=True, cwd=cwd,\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        bufsize=1, text=True,\n",
    "    )\n",
    "    for line in proc.stdout:\n",
    "        print(line, end=\"\", flush=True)\n",
    "    proc.wait()\n",
    "    if proc.returncode != 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Command exited with code {proc.returncode}\")\n",
    "    return proc.returncode\n",
    "\n",
    "# ---------- Claude streaming helper ----------\n",
    "def _display_claude_line(line):\n",
    "    \"\"\"Parse a single stream-json line from Claude CLI and print progress.\"\"\"\n",
    "    if not line.strip():\n",
    "        return\n",
    "    try:\n",
    "        data = json.loads(line)\n",
    "        msg_type = data.get('type', '')\n",
    "        subtype = data.get('subtype', '')\n",
    "\n",
    "        if msg_type == 'system':\n",
    "            if subtype == 'init':\n",
    "                session_id = data.get('session_id', '')[:8]\n",
    "                print(f\"  ü§ñ Session started: {session_id}...\", flush=True)\n",
    "            elif subtype != 'transcript':\n",
    "                print(f\"  ‚öôÔ∏è  System: {subtype}\", flush=True)\n",
    "\n",
    "        elif msg_type == 'assistant':\n",
    "            message = data.get('message', {})\n",
    "            for block in message.get('content', []):\n",
    "                block_type = block.get('type', '')\n",
    "                if block_type == 'thinking':\n",
    "                    text = block.get('thinking', '')[:100]\n",
    "                    print(f\"  üí≠ Thinking: {text}...\", flush=True)\n",
    "                elif block_type == 'text':\n",
    "                    lines = block.get('text', '').strip().split('\\n')\n",
    "                    for tl in lines[:5]:\n",
    "                        if tl.strip():\n",
    "                            print(f\"  {tl}\", flush=True)\n",
    "                    if len(lines) > 5:\n",
    "                        print(f\"  ... ({len(lines) - 5} more lines)\", flush=True)\n",
    "                elif block_type == 'tool_use':\n",
    "                    tool_name = block.get('name', 'unknown')\n",
    "                    tool_input = block.get('input', {})\n",
    "                    if tool_name == 'Bash':\n",
    "                        print(f\"  üîß Bash: {tool_input.get('command', '')[:80]}\", flush=True)\n",
    "                    elif tool_name in ('Read', 'Write', 'Edit'):\n",
    "                        print(f\"  üìñ {tool_name}: {tool_input.get('file_path', '')}\", flush=True)\n",
    "                    elif tool_name.startswith('mcp__'):\n",
    "                        print(f\"  üîå MCP: {tool_name}\", flush=True)\n",
    "                    else:\n",
    "                        print(f\"  üîß {tool_name}\", flush=True)\n",
    "\n",
    "        elif msg_type == 'user':\n",
    "            for block in data.get('message', {}).get('content', []):\n",
    "                if block.get('type') == 'tool_result':\n",
    "                    if block.get('is_error', False):\n",
    "                        err = block.get('content', '')\n",
    "                        err = err[:100] if isinstance(err, str) else str(err)[:100]\n",
    "                        print(f\"  ‚ùå Error: {err}\", flush=True)\n",
    "                    else:\n",
    "                        content = block.get('content', '')\n",
    "                        if isinstance(content, str) and content.strip():\n",
    "                            first = content.strip().split('\\n')[0][:80]\n",
    "                            if first:\n",
    "                                print(f\"  ‚úÖ Result: {first}\", flush=True)\n",
    "                        else:\n",
    "                            print(f\"  ‚úÖ Done\", flush=True)\n",
    "\n",
    "        elif msg_type == 'result':\n",
    "            if subtype == 'success':\n",
    "                print(f\"  ‚úÖ Completed successfully\", flush=True)\n",
    "            elif subtype == 'error':\n",
    "                print(f\"  ‚ùå Error: {data.get('error', 'Unknown')}\", flush=True)\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        if line.strip():\n",
    "            print(f\"  {line}\", flush=True)\n",
    "\n",
    "\n",
    "def run_claude(prompt, allowed_tools=None, cwd=None):\n",
    "    \"\"\"Run Claude CLI with real-time streaming output.\n",
    "\n",
    "    Args:\n",
    "        prompt: The prompt text to send to Claude (passed via stdin).\n",
    "        allowed_tools: Comma-separated tool names, e.g. \"Bash,Read,Write\".\n",
    "        cwd: Working directory for the claude process.\n",
    "\n",
    "    Returns:\n",
    "        Process return code (0 = success).\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        \"claude\",\n",
    "        \"--model\", CLAUDE_MODEL,\n",
    "        \"-p\", \"-\",\n",
    "        \"--output-format\", \"stream-json\",\n",
    "        \"--verbose\",\n",
    "        \"--dangerously-skip-permissions\",\n",
    "    ]\n",
    "    if allowed_tools:\n",
    "        cmd += [\"--allowedTools\", allowed_tools]\n",
    "\n",
    "    print(f\"  ü§ñ Claude model: {CLAUDE_MODEL}\")\n",
    "    print(f\"  üìã Tools: {allowed_tools or 'all'}\")\n",
    "    print(f\"  \" + \"-\" * 58)\n",
    "\n",
    "    proc = subprocess.Popen(\n",
    "        cmd, cwd=cwd,\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True, bufsize=1,\n",
    "    )\n",
    "    proc.stdin.write(prompt)\n",
    "    proc.stdin.close()\n",
    "\n",
    "    while True:\n",
    "        if proc.poll() is not None:\n",
    "            # Drain remaining output\n",
    "            for line in (proc.stdout.read() or '').split('\\n'):\n",
    "                _display_claude_line(line)\n",
    "            for line in (proc.stderr.read() or '').split('\\n'):\n",
    "                if line.strip():\n",
    "                    print(f\"  ‚öôÔ∏è  {line}\", flush=True)\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            readable, _, _ = select.select([proc.stdout, proc.stderr], [], [], 0.1)\n",
    "        except (ValueError, OSError):\n",
    "            break\n",
    "\n",
    "        for stream in readable:\n",
    "            line = stream.readline()\n",
    "            if line:\n",
    "                if stream == proc.stdout:\n",
    "                    _display_claude_line(line.rstrip('\\n'))\n",
    "                else:\n",
    "                    if line.strip():\n",
    "                        print(f\"  ‚öôÔ∏è  {line.rstrip()}\", flush=True)\n",
    "\n",
    "    rc = proc.wait()\n",
    "    print(f\"  \" + \"-\" * 58)\n",
    "    if rc != 0:\n",
    "        print(f\"  ‚ö†Ô∏è  Claude exited with code {rc}\")\n",
    "    return rc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and verify the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---------- Paths ----------\n# Auto-detect REPO_DIR: walk up from notebook location to find project root\n_nb_dir = os.path.abspath(\"\")\nif os.path.basename(_nb_dir) == \"notebooks\":\n    REPO_DIR = os.path.dirname(_nb_dir)\nelse:\n    REPO_DIR = _nb_dir\n\n# ---------- Load API key from .env if not set ----------\nif not ANTHROPIC_API_KEY:\n    _env_file = os.path.join(REPO_DIR, \".env\")\n    if os.path.exists(_env_file):\n        with open(_env_file) as f:\n            for line in f:\n                line = line.strip()\n                if line.startswith(\"ANTHROPIC_API_KEY=\"):\n                    ANTHROPIC_API_KEY = line.split(\"=\", 1)[1].strip().strip(\"\\\"'\")\n                    break\n        if ANTHROPIC_API_KEY:\n            print(f\"Loaded ANTHROPIC_API_KEY from {_env_file}\")\n\nif ANTHROPIC_API_KEY:\n    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n    print(f\"Using API key (ends with ...{ANTHROPIC_API_KEY[-4:]})\")\nelse:\n    print(\"No API key set ‚Äî Claude CLI will use your logged-in account.\")\n\n# ---------- Data paths ----------\nDATA_DIR    = os.path.join(REPO_DIR, \"data\", PROTEIN_NAME)\nRESULTS_DIR = os.path.join(REPO_DIR, \"results\", PROTEIN_NAME)\n\nWT_FASTA = os.path.join(DATA_DIR, \"wt.fasta\")\nDATA_CSV = os.path.join(DATA_DIR, \"data.csv\")\n\nos.makedirs(DATA_DIR, exist_ok=True)\n\nif use_example_data:\n    print(f\"Will use bundled example data for {PROTEIN_NAME}\")\nelse:\n    print(f\"Please place your wt.fasta and data.csv files in:\\n  {DATA_DIR}\")\n    input(\"Press Enter once the files are in place...\")\n    assert os.path.exists(WT_FASTA), f\"Missing {WT_FASTA} ‚Äî please provide wt.fasta\"\n    assert os.path.exists(DATA_CSV),  f\"Missing {DATA_CSV} ‚Äî please provide data.csv\"\n\nprint(f\"\\nCLAUDE_MODEL : {CLAUDE_MODEL}\")\nprint(f\"PROTEIN_NAME : {PROTEIN_NAME}\")\nprint(f\"REPO_DIR     : {REPO_DIR}\")\nprint(f\"DATA_DIR     : {DATA_DIR}\")\nprint(f\"RESULTS_DIR  : {RESULTS_DIR}\")\nprint(f\"WT_FASTA     : {WT_FASTA}\")\nprint(f\"DATA_CSV     : {DATA_CSV}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install & Register MCPs\n",
    "\n",
    "> **Tip ‚Äî Local Docker builds:** Some Docker MCP images (e.g. `esm_mcp`) are large and may\n",
    "> time out during `pmcp install`. You can build and register them locally instead:\n",
    "> ```bash\n",
    "> cd tool-mcps/esm_mcp && docker build -t esm_mcp:latest . && cd ../..\n",
    "> claude mcp add esm_mcp -- docker run -i --rm --gpus all --ipc=host -v $PWD:$PWD esm_mcp:latest\n",
    "> ```\n",
    "> Replace `esm_mcp` with any other Docker MCP name (`plmc_mcp`, `ev_onehot_mcp`, `prottrans_mcp`) as needed.\n",
    "> The install cell below will skip MCPs that are already registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, time\n_t0 = time.time()\n\n# Install ProteinMCP if not already present\nif run_cmd(\"which pmcp\") != 0:\n    run_cmd(f\"pip install -e {REPO_DIR}\")\n    run_cmd(f\"pip install -r {REPO_DIR}/requirements.txt\")\nelse:\n    print(\"ProteinMCP already installed.\")\n\n# Install Claude Code if not already present\nif run_cmd(\"which claude\") != 0:\n    run_cmd(\"npm install -g @anthropic-ai/claude-code\")\nelse:\n    print(\"Claude Code already installed.\")\n\n# Verify Docker is available (required for Docker MCPs)\nif run_cmd(\"docker --version\") != 0:\n    print(\"WARNING: Docker not found. Docker MCPs will not work.\")\nelse:\n    print(\"Docker found.\")\n\nprint(\"ProteinMCP & Claude Code ready.\")\nprint(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All MCPs already registered. Skipping installation.\n",
      "\n",
      "==================================================\n",
      "Checking MCP server health...\n",
      "\n",
      "plmc_mcp: docker run -i --rm --gpus all --ipc=host -v /home/xux/Desktop/AgentMCP/ProteinMCP:/home/xux/Desktop/AgentMCP/ProteinMCP ghcr.io/macromnex/plmc_mcp:latest - ‚úì Connected\n",
      "msa_mcp: python /mnt/data/done_projects/2026/BioMolMCP/ProteinMCP/tool-mcps/msa_mcp/src/server.py - ‚úì Connected\n",
      "ev_onehot_mcp: docker run -i --rm --gpus all --ipc=host -v /home/xux/Desktop/AgentMCP/ProteinMCP:/home/xux/Desktop/AgentMCP/ProteinMCP ghcr.io/macromnex/ev_onehot_mcp:latest - ‚úì Connected\n",
      "prottrans_mcp: docker run -i --rm --gpus all --ipc=host -v /home/xux/Desktop/AgentMCP/ProteinMCP:/home/xux/Desktop/AgentMCP/ProteinMCP ghcr.io/macromnex/prottrans_mcp:latest - ‚úì Connected\n",
      "boltzgen_mcp: docker run -i --rm --gpus all --ipc=host -v /home/xux/Desktop/AgentMCP/ProteinMCP:/home/xux/Desktop/AgentMCP/ProteinMCP boltzgen_mcp:latest - ‚úì Connected\n",
      "bindcraft_mcp: docker run -i --rm --gpus all --ipc=host -v /home/xux/Desktop/AgentMCP/ProteinMCP:/home/xux/Desktop/AgentMCP/ProteinMCP bindcraft_mcp:latest - ‚úì Connected\n",
      "esm_mcp: docker run -i --rm --gpus all --ipc=host -v /home/xux/Desktop/AgentMCP/ProteinMCP:/home/xux/Desktop/AgentMCP/ProteinMCP esm_mcp:latest - ‚úì Connected\n",
      "\n",
      "Elapsed: 30.9s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "_t0 = time.time()\n",
    "\n",
    "# Required MCPs for fitness modeling workflow\n",
    "REQUIRED_MCPS = [\"msa_mcp\", \"plmc_mcp\", \"ev_onehot_mcp\", \"esm_mcp\", \"prottrans_mcp\"]\n",
    "\n",
    "# Check which MCPs are already registered\n",
    "_registered = subprocess.run([\"claude\", \"mcp\", \"list\"], capture_output=True, text=True).stdout\n",
    "_to_install = [m for m in REQUIRED_MCPS if m not in _registered]\n",
    "\n",
    "if _to_install:\n",
    "    print(f\"Installing {len(_to_install)} MCPs: {_to_install}\")\n",
    "    _skipped = [m for m in REQUIRED_MCPS if m in _registered]\n",
    "    if _skipped:\n",
    "        print(f\"Skipping already registered: {_skipped}\")\n",
    "\n",
    "    def _install_mcp(mcp_name):\n",
    "        \"\"\"Install a single MCP, capturing output for parallel execution.\"\"\"\n",
    "        result = subprocess.run(\n",
    "            [\"pmcp\", \"install\", mcp_name],\n",
    "            capture_output=True, text=True, cwd=REPO_DIR, timeout=1800\n",
    "        )\n",
    "        return mcp_name, result.returncode, result.stdout, result.stderr\n",
    "\n",
    "    # Install (pull Docker images) and register MCPs in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(_to_install)) as executor:\n",
    "        futures = {executor.submit(_install_mcp, name): name for name in _to_install}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            name, rc, stdout, stderr = future.result()\n",
    "            status = \"OK\" if rc == 0 else \"FAILED\"\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"  {name}: {status}\")\n",
    "            # Show last few meaningful lines\n",
    "            for line in stdout.strip().splitlines()[-5:]:\n",
    "                if line.strip():\n",
    "                    print(f\"    {line.strip()}\")\n",
    "            if rc != 0 and stderr:\n",
    "                print(f\"    stderr: {stderr.strip()[:200]}\")\n",
    "else:\n",
    "    print(\"All MCPs already registered. Skipping installation.\")\n",
    "\n",
    "# Verify MCP status\n",
    "print(f\"\\n{'='*50}\")\n",
    "run_cmd(\"claude mcp list\", cwd=REPO_DIR)\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 ‚Äî Setup Results Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/wt.fasta ‚Üí /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/wt.fasta\n",
      "Copied /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/data.csv ‚Üí /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/data.csv\n",
      "\n",
      "Results directory ready: /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V\n",
      "Files: ['figures', 'data.csv', 'wt.fasta']\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "EXAMPLE_DIR = os.path.join(REPO_DIR, \"examples\", \"case1_fitness_modeling\")\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "if use_example_data:\n",
    "    # Copy bundled example data into DATA_DIR\n",
    "    for fname in [\"wt.fasta\", \"data.csv\"]:\n",
    "        src = os.path.join(EXAMPLE_DIR, fname)\n",
    "        dst = os.path.join(DATA_DIR, fname)\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copy2(src, dst)\n",
    "            print(f\"Copied {src} ‚Üí {dst}\")\n",
    "\n",
    "# Copy input files to RESULTS_DIR (needed by training tools)\n",
    "for fname in [\"wt.fasta\", \"data.csv\"]:\n",
    "    src = os.path.join(DATA_DIR, fname)\n",
    "    dst = os.path.join(RESULTS_DIR, fname)\n",
    "    if not os.path.exists(dst):\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"Copied {src} ‚Üí {dst}\")\n",
    "\n",
    "# Verify\n",
    "assert os.path.exists(os.path.join(RESULTS_DIR, \"wt.fasta\")), \"wt.fasta missing in RESULTS_DIR\"\n",
    "assert os.path.exists(os.path.join(RESULTS_DIR, \"data.csv\")),  \"data.csv missing in RESULTS_DIR\"\n",
    "print(f\"\\nResults directory ready: {RESULTS_DIR}\")\n",
    "print(f\"Files: {os.listdir(RESULTS_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 ‚Äî Generate MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "step1_msa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ü§ñ Claude model: claude-haiku-4-5-20251001\n",
      "  üìã Tools: mcp__msa_mcp__generate_msa,Bash,Read,Write\n",
      "  ----------------------------------------------------------\n",
      "  ü§ñ Session started: e627d304...\n",
      "  üí≠ Thinking: The user wants me to:\n",
      "1. Read the wild-type FASTA sequence from `/home/xux/Desktop/AgentMCP/ProteinM...\n",
      "  I'll help you obtain the MSA for TEVp_S219V. Let me start by reading the wild-type FASTA file.\n",
      "  üìñ Read: /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/wt.fasta\n",
      "  ‚úÖ Result: 1‚Üí>TEVp_S219V\n",
      "  üí≠ Thinking: Good! I have the wild-type sequence. Now I need to call the MSA MCP to generate the multiple sequenc...\n",
      "  Now I'll generate the MSA using the MSA MCP with the protein sequence and save it to the specified absolute path.\n",
      "  üîå MCP: mcp__msa_mcp__generate_msa\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"ticket_id\":\"QdpEbHF_WWqv5Yl4Gt-QzSLNjfAytF1naoqnwQ\",\"output\n",
      "  üí≠ Thinking: Perfect! The MSA has been generated successfully. The results show:\n",
      "- Status: success\n",
      "- Output file ...\n",
      "  Perfect! The MSA has been generated successfully for TEVp_S219V. Here's a summary:\n",
      "  **MSA Generation Results:**\n",
      "  - ‚úì Status: Success\n",
      "  - **Output file:** `/home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/TEVp_S219V.a3m`\n",
      "  ... (5 more lines)\n",
      "  ‚úÖ Completed successfully\n",
      "  ----------------------------------------------------------\n",
      "\n",
      "MSA generated: /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/TEVp_S219V.a3m\n",
      "Elapsed: 38.8s\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "Can you obtain the MSA for {PROTEIN_NAME} from {WT_FASTA} using msa mcp \\\n",
    "and save it to {RESULTS_DIR}/{PROTEIN_NAME}.a3m.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt,\n",
    "    allowed_tools=\"mcp__msa_mcp__generate_msa,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# Verify output\n",
    "msa_file = f\"{RESULTS_DIR}/{PROTEIN_NAME}.a3m\"\n",
    "assert os.path.exists(msa_file), f\"MSA file not found: {msa_file}\"\n",
    "print(f\"\\nMSA generated: {msa_file}\")\n",
    "print(f\"Elapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 ‚Äî Build PLMC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "step2_plmc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ü§ñ Claude model: claude-haiku-4-5-20251001\n",
      "  üìã Tools: mcp__plmc_mcp__plmc_convert_a3m_to_a2m,mcp__plmc_mcp__plmc_generate_model,Bash,Read,Write\n",
      "  ----------------------------------------------------------\n",
      "  ü§ñ Session started: 4f4cc56e...\n",
      "  üí≠ Thinking: The user wants me to:\n",
      "1. Convert an A3M file to A2M format using plmc_mcp\n",
      "2. Generate a PLMC model u...\n",
      "  I'll help you build an EV model using PLMC. Let me break this down into steps:\n",
      "  1. Convert A3M to A2M format\n",
      "  2. Generate the PLMC model\n",
      "  3. Create the symlinks\n",
      "  ... (2 more lines)\n",
      "  üîå MCP: mcp__plmc_mcp__plmc_convert_a3m_to_a2m\n",
      "  ‚úÖ Result: {\"message\":\"Successfully converted A3M to A2M format and cleaned query gaps\",\"re\n",
      "  üí≠ Thinking: Great! The A3M to A2M conversion was successful. Now I need to generate the PLMC model using the con...\n",
      "  Now I'll generate the PLMC model:\n",
      "  üîå MCP: mcp__plmc_mcp__plmc_generate_model\n",
      "  ‚ùå Error: Error calling tool 'plmc_generate_model': Focus sequence identifier must be provided\n",
      "  üí≠ Thinking: It seems the focus_seq_id is required. Let me check what the focus sequence ID should be. It's typic...\n",
      "  I need to determine the focus sequence ID from the alignment file. Let me check:\n",
      "  üìñ Read: /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/plmc/TEVp_S219V.a2m\n",
      "  ‚úÖ Result: 1‚Üí>TEVp_S219V_MSA\n",
      "  üí≠ Thinking: I can see from the A2M file that the first sequence ID is \"TEVp_S219V_MSA\". This is likely the focus...\n",
      "  Perfect! The focus sequence ID is \"TEVp_S219V_MSA\". Let me generate the model with that:\n",
      "  üîå MCP: mcp__plmc_mcp__plmc_generate_model\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "I have created an a3m file in {RESULTS_DIR}/{PROTEIN_NAME}.a3m. \\\n",
    "Can you help build an EV model using plmc mcp and save it to {RESULTS_DIR}/plmc directory. \\\n",
    "The wild-type sequence is {WT_FASTA}.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "After building the model, create symlinks in {RESULTS_DIR}/plmc/:\n",
    "  ln -sf {PROTEIN_NAME}.model_params uniref100.model_params\n",
    "  ln -sf {PROTEIN_NAME}.EC uniref100.EC\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt,\n",
    "    allowed_tools=\"mcp__plmc_mcp__plmc_convert_a3m_to_a2m,mcp__plmc_mcp__plmc_generate_model,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# Verify outputs\n",
    "plmc_dir = f\"{RESULTS_DIR}/plmc\"\n",
    "assert os.path.exists(f\"{plmc_dir}/uniref100.model_params\"), \"PLMC model_params symlink missing\"\n",
    "assert os.path.exists(f\"{plmc_dir}/uniref100.EC\"), \"PLMC EC symlink missing\"\n",
    "print(f\"\\nPLMC model built: {os.listdir(plmc_dir)}\")\n",
    "print(f\"Elapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äî Build EV+OneHot Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "step3_ev_onehot"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ü§ñ Claude model: claude-haiku-4-5-20251001\n",
      "  üìã Tools: mcp__ev_onehot_mcp__ev_onehot_train_fitness_predictor,Bash,Read,Write\n",
      "  ----------------------------------------------------------\n",
      "  ü§ñ Session started: 99d8cc47...\n",
      "  üí≠ Thinking: The user wants me to build an EV+OneHot model using the ev_onehot_mcp. They've provided:\n",
      "1. PLMC mod...\n",
      "  I'll help you build an EV+OneHot model using the ev_onehot_mcp. Let me call the training function with all absolute paths.\n",
      "  üîå MCP: mcp__ev_onehot_mcp__ev_onehot_train_fitness_predictor\n",
      "  ‚úÖ Result: {\"message\":\"5-fold CV: 0.261 ¬± 0.084, Final model trained on 384 samples\",\"artif\n",
      "  üí≠ Thinking: Great! The EV+OneHot model training was successful. The function returned:\n",
      "1. The trained model: rid...\n",
      "  Perfect! I've successfully built the EV+OneHot model. Here's what was generated:\n",
      "  **Model Training Results:**\n",
      "  - **5-fold Cross-Validation Spearman Correlation:** 0.261 ¬± 0.084\n",
      "  - **Final Model:** Trained on 384 samples\n",
      "  ... (11 more lines)\n",
      "  ‚úÖ Completed successfully\n",
      "  ----------------------------------------------------------\n",
      "\n",
      "EV+OneHot model trained. Metrics:\n",
      "           stage fold  n_train  n_test  spearman_correlation\n",
      "cross_validation    1      307      77              0.420947\n",
      "cross_validation    2      307      77              0.241468\n",
      "cross_validation    3      307      77              0.246750\n",
      "cross_validation    4      307      77              0.174835\n",
      "cross_validation    5      308      76              0.221365\n",
      "      cv_summary mean      307      76              0.261073\n",
      "      cv_summary  std        0       0              0.083861\n",
      "     final_model  all      384       0              0.572203\n",
      "\n",
      "Elapsed: 47.7s\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "I have created a plmc model in directory {RESULTS_DIR}/plmc. \\\n",
    "Can you help build an EV+OneHot model using ev_onehot_mcp and save it to {RESULTS_DIR}/ directory. \\\n",
    "The wild-type sequence is {RESULTS_DIR}/wt.fasta, and the dataset is {RESULTS_DIR}/data.csv.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt,\n",
    "    allowed_tools=\"mcp__ev_onehot_mcp__ev_onehot_train_fitness_predictor,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# Verify output\n",
    "assert os.path.exists(f\"{RESULTS_DIR}/metrics_summary.csv\"), \"EV+OneHot metrics not found\"\n",
    "print(f\"\\nEV+OneHot model trained. Metrics:\")\n",
    "import pandas as pd\n",
    "print(pd.read_csv(f\"{RESULTS_DIR}/metrics_summary.csv\").to_string(index=False))\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äî Build ESM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "step4_esm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ESM2-650M: extracting embeddings & training models\n",
      "============================================================\n",
      "  ü§ñ Claude model: claude-haiku-4-5-20251001\n",
      "  üìã Tools: mcp__esm_mcp__extract_protein_embeddings,mcp__esm_mcp__esm_train_fitness_model,Bash,Read,Write\n",
      "  ----------------------------------------------------------\n",
      "  ü§ñ Session started: ad21d78f...\n",
      "  üí≠ Thinking: The user wants me to:\n",
      "1. Train ESM models for data in `/home/xux/Desktop/AgentMCP/ProteinMCP/results...\n",
      "  I'll help you train ESM models for the TEVp_S219V dataset. Let me start by checking the directory structure and then proceed with extracting embeddings and training models.\n",
      "  üîß Bash: ls -la /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/\n",
      "  ‚úÖ Result: total 10308\n",
      "  üí≠ Thinking: Good! I can see that:\n",
      "1. The embeddings already exist: `data.csv.esm2_t33_650M_UR50D.npy` (the esm2_...\n",
      "  üîß Bash: ls -la /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/esm2_650M_svr/\n",
      "  ‚úÖ Result: total 24\n",
      "  üí≠ Thinking: I see that the models are already trained (they have cv_results.csv, training_summary.csv, and final...\n",
      "  The embeddings already exist (I can see `data.csv.esm2_t33_650M_UR50D.npy`), and there are already trained models in place. However, I'll proceed with training as requested. Let me check the data structure first:\n",
      "  üîß Bash: head -5 /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/data.csv\n",
      "  ‚úÖ Result: AACombo,AA1,AA2,AA3,AA4,fitness,active,muts,n_mut,mutations,seq,n_muts,log_fitne\n",
      "  üí≠ Thinking: Good, I can see the data.csv has columns including `log_fitness` which is the target column (based o...\n",
      "  Now I'll train the ESM models with the three head models (svr, xgboost, knn). Since these are independent operations, I'll run them in parallel:\n",
      "  üîå MCP: mcp__esm_mcp__esm_train_fitness_model\n",
      "  üîå MCP: mcp__esm_mcp__esm_train_fitness_model\n",
      "  üîå MCP: mcp__esm_mcp__esm_train_fitness_model\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  üí≠ Thinking: Great! All three models have been successfully trained. Let me summarize the results for the user....\n",
      "  Perfect! I've successfully trained all three ESM models for the TEVp_S219V dataset. Here's a summary of the results:\n",
      "  ## Training Results\n",
      "  ### 1. **esm2_650M_svr** (Support Vector Regression)\n",
      "  ... (30 more lines)\n",
      "  ‚úÖ Completed successfully\n",
      "  ----------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "ESM2-3B: extracting embeddings & training models\n",
      "============================================================\n",
      "  ü§ñ Claude model: claude-haiku-4-5-20251001\n",
      "  üìã Tools: mcp__esm_mcp__extract_protein_embeddings,mcp__esm_mcp__esm_train_fitness_model,Bash,Read,Write\n",
      "  ----------------------------------------------------------\n",
      "  ü§ñ Session started: 305bc790...\n",
      "  üí≠ Thinking: The user is asking me to:\n",
      "1. Train ESM models for data in `/home/xux/Desktop/AgentMCP/ProteinMCP/res...\n",
      "  I'll help you train ESM models for the TEVp_S219V data. Let me first check the data structure and then extract embeddings if needed, then train the models.\n",
      "  üîß Bash: ls -la \"/home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/\"\n",
      "  üîß Bash: find \"/home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/\" -name \"*.csv\" -\n",
      "  ‚ùå Error: Exit code 2\n",
      "ls: cannot access '/home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/': No such f\n",
      "  ‚ùå Error: <tool_use_error>Sibling tool call errored</tool_use_error>\n",
      "  üí≠ Thinking: The directory doesn't exist. Let me check what directories exist in the results folder....\n",
      "  üîß Bash: ls -la \"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\" 2>/dev/null || echo \"Res\n",
      "  ‚úÖ Result: total 28\n",
      "  üí≠ Thinking: The TEVp_S219V directory doesn't exist. Let me check if there's any data in the existing directories...\n",
      "  The `/home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/` directory doesn't exist. Let me check what directories are available:\n",
      "  üîß Bash: find \"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\" -name \"*.csv\" -type f 2>/d\n",
      "  ‚úÖ Result: /home/xux/Desktop/AgentMCP/ProteinMCP/results/nanobody_design/designs/intermedia\n",
      "  üí≠ Thinking: It seems the TEVp_S219V directory doesn't exist. I should ask the user for clarification about where...\n",
      "  It looks like the `/home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/` directory doesn't exist. Could you please:\n",
      "  1. **Confirm the correct path** to your data, or\n",
      "  2. **Provide the data file** (CSV with sequences and fitness values) so I can set up the training\n",
      "  ... (6 more lines)\n",
      "  ‚úÖ Completed successfully\n",
      "  ----------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "ESM model outputs:\n",
      "============================================================\n",
      "  ‚úó esm2_650M_svr: NOT FOUND\n",
      "  ‚úó esm2_650M_xgboost: NOT FOUND\n",
      "  ‚úó esm2_650M_knn: NOT FOUND\n",
      "  ‚úó esm2_3B_svr: NOT FOUND\n",
      "  ‚úó esm2_3B_xgboost: NOT FOUND\n",
      "  ‚úó esm2_3B_knn: NOT FOUND\n",
      "\n",
      "Elapsed: 89.9s\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "# --- 4.1: ESM2-650M (extract embeddings + train svr/xgboost/knn) ---\n",
    "print(\"=\"*60)\n",
    "print(\"ESM2-650M: extracting embeddings & training models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt_650m = f\"\"\"\\\n",
    "Can you help train ESM models for data in {RESULTS_DIR}/ and save them to \\\n",
    "{RESULTS_DIR}/esm2_650M_{{head_model}} using the esm_mcp server with svr, xgboost, \\\n",
    "and knn as the head models.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "Obtain the embeddings if they are not created.\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt_650m,\n",
    "    allowed_tools=\"mcp__esm_mcp__extract_protein_embeddings,mcp__esm_mcp__esm_train_fitness_model,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# --- 4.2: ESM2-3B (extract embeddings + train svr/xgboost/knn) ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ESM2-3B: extracting embeddings & training models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt_3b = f\"\"\"\\\n",
    "Can you help train ESM models for data in {RESULTS_DIR}/ and save them to \\\n",
    "{RESULTS_DIR}/esm2_3B_{{head_model}} using the esm mcp server with svr, xgboost, \\\n",
    "and knn as the head models and esm2_t36_3B_UR50D as the backbone.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "Obtain the embeddings if they are not created.\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt_3b,\n",
    "    allowed_tools=\"mcp__esm_mcp__extract_protein_embeddings,mcp__esm_mcp__esm_train_fitness_model,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ESM model outputs:\")\n",
    "print(\"=\"*60)\n",
    "for backbone in [\"esm2_650M\", \"esm2_3B\"]:\n",
    "    for head in [\"svr\", \"xgboost\", \"knn\"]:\n",
    "        d = f\"{RESULTS_DIR}/{backbone}_{head}\"\n",
    "        if os.path.isdir(d):\n",
    "            print(f\"  ‚úì {backbone}_{head}: {os.listdir(d)}\")\n",
    "        else:\n",
    "            print(f\"  ‚úó {backbone}_{head}: NOT FOUND\")\n",
    "\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 ‚Äî Build ProtTrans Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "step5_prottrans"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ProtTrans: extracting embeddings & training models\n",
      "============================================================\n",
      "  ü§ñ Claude model: claude-haiku-4-5-20251001\n",
      "  üìã Tools: mcp__prottrans_mcp__prottrans_extract_embeddings,mcp__prottrans_mcp__prottrans_train_fitness_model,Bash,Read,Write\n",
      "  ----------------------------------------------------------\n",
      "  ü§ñ Session started: 8ef4f8a9...\n",
      "  üí≠ Thinking: The user wants me to:\n",
      "1. Train ProtTrans models for data in /home/xux/Desktop/AgentMCP/ProteinMCP/re...\n",
      "  I'll help you train ProtTrans models for the TEVp_S219V dataset. Let me start by exploring the data directory and then create embeddings and train models for all combinations.\n",
      "  üîß Bash: ls -la /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/\n",
      "  ‚ùå Error: Exit code 2\n",
      "ls: cannot access '/home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/': No such f\n",
      "  üí≠ Thinking: The directory doesn't exist. Let me check if the results directory exists at all....\n",
      "  üîß Bash: ls -la /home/xux/Desktop/AgentMCP/ProteinMCP/results/ 2>/dev/null || echo \"resul\n",
      "  ‚úÖ Result: total 28\n",
      "  üí≠ Thinking: The TEVp_S219V directory doesn't exist yet. The user might want me to create it or they may have a d...\n",
      "  The TEVp_S219V directory doesn't exist yet. Let me search for any relevant data files in the project:\n",
      "  üîß Glob\n",
      "  ‚úÖ Result: /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/data.csv\n",
      "  üîß Bash: find /home/xux/Desktop/AgentMCP/ProteinMCP -name \"*TEV*\" -o -name \"*S219V*\" 2>/d\n",
      "  ‚úÖ Result: /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V\n",
      "  üí≠ Thinking: Great! The data file is at /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/data.csv. The user ...\n",
      "  Perfect! I found the data file. Let me examine it and set up the training pipeline:\n",
      "  üìñ Read: /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/data.csv\n",
      "  ‚úÖ Result: 1‚ÜíAACombo,AA1,AA2,AA3,AA4,fitness,active,muts,n_mut,mutations,seq,n_muts,log_fit\n",
      "  üîß Bash: wc -l /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/data.csv && head -1 \n",
      "  ‚úÖ Result: 385 /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/data.csv\n",
      "  üí≠ Thinking: Good! The data has:\n",
      "- 385 rows (including header)\n",
      "- Columns: AACombo, AA1, AA2, AA3, AA4, fitness, a...\n",
      "  Now let me check if embeddings already exist and create the output directory:\n",
      "  üîß Bash: ls -la /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/\n",
      "  ‚úÖ Result: total 144\n",
      "  üîß Bash: mkdir -p /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V && echo \"Outpu\n",
      "  ‚úÖ Result: Output directory created\n",
      "  üí≠ Thinking: Good. The embeddings don't exist yet (I only see data.csv and wt.fasta). Now I need to create embedd...\n",
      "  Now I'll create embeddings for both backbone models and train all combinations. Let me start by creating embeddings for ProtT5-XL:\n",
      "  üîß Skill\n",
      "  ‚úÖ Result: Launching skill: fitness-model\n",
      "  üí≠ Thinking: Now I understand the fitness-model skill and the complete workflow. Based on the user's request, I n...\n",
      "  Now I understand the workflow. Let me create ProtTrans embeddings for both backbone models and then train all 6 model combinations (2 backbones √ó 3 head models). Let me start:\n",
      "  üîß Bash: python3 << 'EOF'\n",
      "import sys\n",
      "sys.path.insert(0, '/home/xux/Desktop/AgentMCP/Prote\n",
      "  ‚úÖ Result: [TIMING] Started step: ProtTrans at 16:06:25\n",
      "  üí≠ Thinking: Good, now let me create embeddings for ProtT5-XL and ProtAlbert. I'll call the prottrans_extract_emb...\n",
      "  Now let me create embeddings for both ProtT5-XL and ProtAlbert:\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_extract_embeddings\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_extract_embeddings\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"csv_path\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"csv_path\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_\n",
      "  üí≠ Thinking: Great! Both embeddings have been created successfully:\n",
      "- ProtT5-XL: 384 sequences with 1024-dimensio...\n",
      "  Excellent! Both embeddings created. Now let me copy the data files to the results directory and train all 6 model combinations:\n",
      "  üîß Bash: \n",
      "# Copy input files to results directory\n",
      "cp /home/xux/Desktop/AgentMCP/ProteinMC\n",
      "  ‚úÖ Result: Files copied to results directory\n",
      "  üí≠ Thinking: Now I need to handle the embeddings location. The embeddings were created in the data directory, but...\n",
      "  üîß Bash: \n",
      "# Copy embeddings to results directory\n",
      "cp -r /home/xux/Desktop/AgentMCP/Protein\n",
      "  ‚úÖ Result: Embeddings copied to results directory\n",
      "  üí≠ Thinking: Good. Now I'll train all 6 model combinations. Let me call the training tool for each combination. T...\n",
      "  Now I'll train all 6 model combinations in parallel. This may take some time:\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_train_fitness_model\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_train_fitness_model\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_train_fitness_model\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_train_fitness_model\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_train_fitness_model\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_train_fitness_model\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  üí≠ Thinking: Good! All 6 models have completed training. I notice that the ProtT5-XL models have CV results (mean...\n",
      "  Great! All 6 models have been trained. I see the ProtT5-XL models completed with CV scores, but the ProtAlbert models show null values. Let me check the summary files and create a results overview:\n",
      "  üîß Bash: \n",
      "python3 << 'EOF'\n",
      "import pandas as pd\n",
      "from pathlib import Path\n",
      "\n",
      "results_dir = Pa\n",
      "  ‚úÖ Result: ======================================================================\n",
      "  üí≠ Thinking: I see that the ProtAlbert models didn't generate valid CV results. Let me check the CV results files...\n",
      "  üìñ Read: /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/ProtAlbert_svr/ProtAlbert_svr_cv_results.csv\n",
      "  ‚úÖ Result: 1‚Üífold,train_spearman,test_spearman,train_size,test_size\n",
      "  üìñ Read: /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/ProtAlbert_svr/training_summary.csv\n",
      "  ‚úÖ Result: 1‚Üístatus,output_dir,cv_results,cv_mean,cv_std,cv_min,cv_max,cv_results_file,n_sa\n",
      "  üí≠ Thinking: The ProtAlbert models returned NaN values for the Spearman correlations. This is likely because the ...\n",
      "  The ProtAlbert models encountered issues (returning NaN for CV scores), likely due to the high-dimensional embeddings (4096 dims). However, the ProtT5-XL models trained successfully. Let me complete the workflow and create a final summary:\n",
      "  üîß Bash: python3 << 'EOF'\n",
      "import sys\n",
      "sys.path.insert(0, '/home/xux/Desktop/AgentMCP/Prote\n",
      "  ‚úÖ Result: [TIMING] Completed step: ProtTrans in 1.37 minutes\n",
      "  üí≠ Thinking: Good. Now let me create a comprehensive final report summarizing what was done and the results....\n",
      "  Perfect! Let me create a final comprehensive summary report:\n",
      "  üîß Bash: \n",
      "cat > /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/TRAINING_SUMMARY\n",
      "  ‚úÖ Result: ================================================================================\n",
      "  üí≠ Thinking: Perfect! Now let me create a final summary showing the directory structure and what was accomplished...\n",
      "  Excellent! Let me verify the final output structure:\n",
      "  üîß Bash: \n",
      "echo \"=== Final Results Directory Structure ===\" && \\\n",
      "ls -lh /home/xux/Desktop/\n",
      "  ‚úÖ Result: === Final Results Directory Structure ===\n",
      "  üí≠ Thinking: Perfect! Everything has been successfully created. Let me create one more summary showing what was a...\n",
      "  Perfect! Here's the final summary:\n",
      "  ## ‚úÖ ProtTrans Model Training Complete\n",
      "  ### Summary\n",
      "  ... (36 more lines)\n",
      "  ‚úÖ Completed successfully\n",
      "  ----------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "ProtTrans model outputs:\n",
      "============================================================\n",
      "  ‚úì ProtT5-XL_svr: ['ProtT5-XL_svr_cv_results.csv', 'final_model', 'training_summary.csv', 'cv_folds']\n",
      "  ‚úì ProtT5-XL_xgboost: ['ProtT5-XL_xgboost_cv_results.csv', 'final_model', 'training_summary.csv', 'cv_folds']\n",
      "  ‚úì ProtT5-XL_knn: ['final_model', 'training_summary.csv', 'ProtT5-XL_knn_cv_results.csv', 'cv_folds']\n",
      "  ‚úì ProtAlbert_svr: ['ProtAlbert_svr_cv_results.csv', 'final_model', 'training_summary.csv', 'cv_folds']\n",
      "  ‚úì ProtAlbert_xgboost: ['ProtAlbert_xgboost_cv_results.csv', 'final_model', 'training_summary.csv', 'cv_folds']\n",
      "  ‚úì ProtAlbert_knn: ['ProtAlbert_knn_cv_results.csv', 'final_model', 'training_summary.csv', 'cv_folds']\n",
      "\n",
      "Elapsed: 163.0s\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ProtTrans: extracting embeddings & training models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "Can you help train ProtTrans models for data in {RESULTS_DIR}/ and save them to \\\n",
    "{RESULTS_DIR}/{{backbone_model}}_{{head_model}} using the prottrans mcp server with \\\n",
    "ProtT5-XL and ProtAlbert as backbone_models and knn, xgboost, and svr as the head models.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "Create the embeddings if they are not created.\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt,\n",
    "    allowed_tools=\"mcp__prottrans_mcp__prottrans_extract_embeddings,mcp__prottrans_mcp__prottrans_train_fitness_model,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ProtTrans model outputs:\")\n",
    "print(\"=\"*60)\n",
    "for backbone in [\"ProtT5-XL\", \"ProtAlbert\"]:\n",
    "    for head in [\"svr\", \"xgboost\", \"knn\"]:\n",
    "        d = f\"{RESULTS_DIR}/{backbone}_{head}\"\n",
    "        if os.path.isdir(d):\n",
    "            print(f\"  ‚úì {backbone}_{head}: {os.listdir(d)}\")\n",
    "        else:\n",
    "            print(f\"  ‚úó {backbone}_{head}: NOT FOUND\")\n",
    "\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 ‚Äî Aggregate Results & Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "troubleshooting"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 6 model results to all_models_comparison.csv\n",
      "\n",
      "  backbone    head  mean_cv_spearman  std_cv_spearman\n",
      " ProtT5-XL xgboost          0.199161         0.078068\n",
      " ProtT5-XL     svr          0.172025         0.083257\n",
      " ProtT5-XL     knn          0.149401         0.040384\n",
      "ProtAlbert     knn               NaN              NaN\n",
      "ProtAlbert     svr               NaN              NaN\n",
      "ProtAlbert xgboost               NaN              NaN\n",
      "Error: No model results found in /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V\n",
      "\n",
      "Visualization failed\n",
      "\n",
      "‚ö†Ô∏è  Command exited with code 1\n",
      "\n",
      "Elapsed: 2.5s\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import pandas as pd\n",
    "_t0 = time.time()\n",
    "\n",
    "# ---- 6.1 Collect and aggregate all model results ----\n",
    "results = []\n",
    "\n",
    "# EV+OneHot ‚Äî metrics_summary.csv (stage/fold format)\n",
    "ev_path = os.path.join(RESULTS_DIR, \"metrics_summary.csv\")\n",
    "if os.path.exists(ev_path):\n",
    "    ev = pd.read_csv(ev_path)\n",
    "    cv_mean = ev[ev[\"fold\"] == \"mean\"][\"spearman_correlation\"].values[0]\n",
    "    cv_std  = ev[ev[\"fold\"] == \"std\"][\"spearman_correlation\"].values[0]\n",
    "    results.append({\"backbone\": \"EV+OneHot\", \"head\": \"ridge\",\n",
    "                    \"mean_cv_spearman\": cv_mean, \"std_cv_spearman\": cv_std})\n",
    "\n",
    "# ESM & ProtTrans ‚Äî training_summary.csv in subdirectories\n",
    "for dir_name in sorted(os.listdir(RESULTS_DIR)):\n",
    "    summary = os.path.join(RESULTS_DIR, dir_name, \"training_summary.csv\")\n",
    "    if not os.path.exists(summary):\n",
    "        continue\n",
    "    df = pd.read_csv(summary)\n",
    "    if \"mean_cv_spearman\" in df.columns:\n",
    "        mean_sp = df[\"mean_cv_spearman\"].values[0]\n",
    "        std_sp  = df[\"std_cv_spearman\"].values[0]\n",
    "    elif \"cv_mean\" in df.columns:\n",
    "        mean_sp = df[\"cv_mean\"].values[0]\n",
    "        std_sp  = df[\"cv_std\"].values[0]\n",
    "    else:\n",
    "        continue\n",
    "    parts = dir_name.rsplit(\"_\", 1)\n",
    "    if len(parts) == 2:\n",
    "        results.append({\"backbone\": parts[0], \"head\": parts[1],\n",
    "                        \"mean_cv_spearman\": mean_sp, \"std_cv_spearman\": std_sp})\n",
    "\n",
    "all_models = pd.DataFrame(results)\n",
    "all_models.to_csv(os.path.join(RESULTS_DIR, \"all_models_comparison.csv\"), index=False)\n",
    "print(f\"Saved {len(results)} model results to all_models_comparison.csv\\n\")\n",
    "print(all_models.sort_values(\"mean_cv_spearman\", ascending=False).to_string(index=False))\n",
    "\n",
    "# ---- 6.2 Generate four-panel visualization ----\n",
    "VIZ_SCRIPT = os.path.join(REPO_DIR, \"workflow-skills\", \"scripts\", \"fitness_modeling_viz.py\")\n",
    "\n",
    "# Install viz deps if needed, then run with system python\n",
    "run_cmd(\"pip install -q matplotlib seaborn scipy Pillow\")\n",
    "run_cmd(f\"python {VIZ_SCRIPT} {RESULTS_DIR}\")\n",
    "\n",
    "# ---- 6.3 Display figure inline ----\n",
    "from IPython.display import display, Image\n",
    "\n",
    "summary_png = os.path.join(RESULTS_DIR, \"figures\", \"fitness_modeling_summary.png\")\n",
    "if os.path.exists(summary_png):\n",
    "    print(\"\\nFour-panel summary:\")\n",
    "    display(Image(filename=summary_png, width=800))\n",
    "else:\n",
    "    # Try individual figures\n",
    "    figs_dir = os.path.join(RESULTS_DIR, \"figures\")\n",
    "    if os.path.isdir(figs_dir):\n",
    "        for f in sorted(os.listdir(figs_dir)):\n",
    "            if f.endswith(\".png\"):\n",
    "                print(f\"\\n{f}:\")\n",
    "                display(Image(filename=os.path.join(figs_dir, f), width=500))\n",
    "    else:\n",
    "        print(\"No figures generated ‚Äî check logs above.\")\n",
    "\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 ‚Äî Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: TEVp_S219V/ (stored 0%)\n",
      "  adding: TEVp_S219V/figures/ (stored 0%)\n",
      "\n",
      "Results available at: /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V\n",
      "Zipped archive: /home/xux/Desktop/AgentMCP/ProteinMCP/TEVp_S219V_results.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Summary table ----\n",
    "csv_path = os.path.join(RESULTS_DIR, \"all_models_comparison.csv\")\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path).sort_values(\"mean_cv_spearman\", ascending=False)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL PERFORMANCE SUMMARY (5-fold CV Spearman \\u03c1)\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, row in df.iterrows():\n",
    "        print(f\"  {row['backbone']:15} + {row['head']:8}: \"\n",
    "              f\"{row['mean_cv_spearman']:.3f} \\u00b1 {row['std_cv_spearman']:.3f}\")\n",
    "    print(\"=\" * 60)\n",
    "    best = df.iloc[0]\n",
    "    print(f\"  Best: {best['backbone']} ({best['head']}) \\u2014 \\u03c1 = {best['mean_cv_spearman']:.3f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# ---- Zip results ----\n",
    "zip_path = os.path.join(REPO_DIR, f\"{PROTEIN_NAME}_results.zip\")\n",
    "run_cmd(f'cd \"{os.path.dirname(RESULTS_DIR)}\" && zip -r \"{zip_path}\" \"{PROTEIN_NAME}\"')\n",
    "print(f\"\\nResults available at: {RESULTS_DIR}\")\n",
    "print(f\"Zipped archive: {zip_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Instructions & Troubleshooting\n",
    "\n",
    "### Data Format\n",
    "Your `data.csv` must contain at minimum:\n",
    "- **`seq`** ‚Äî Full protein sequence\n",
    "- **`log_fitness`** ‚Äî Log-transformed fitness value (target)\n",
    "\n",
    "Your `wt.fasta` should contain the wild-type reference sequence in standard FASTA format.\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| `uniref100.model_params not found` | Re-run Step 2 ‚Äî symlinks may not have been created |\n",
    "| `wt.fasta not found` in EV+OneHot | Ensure wt.fasta is in RESULTS_DIR (Step 0) |\n",
    "| ESM embeddings extraction fails | Check that `esm_mcp` Docker image is pulled (`pmcp status`) |\n",
    "| GPU Out of Memory | Try smaller backbone (ESM2-650M instead of 3B) |\n",
    "| Low Spearman correlation | Check data quality; ensure proper log-transformation |\n",
    "| MCP not found | Run `pmcp install <mcp_name>` to pull the Docker image and register |\n",
    "| Docker image not pulled | Run `pmcp install <mcp_name>` to pull the image |\n",
    "\n",
    "### Model Performance Reference\n",
    "\n",
    "| Model | Typical CV Spearman | Best Use |\n",
    "|-------|-------------------|----------|\n",
    "| EV+OneHot | 0.20‚Äì0.35 | Baseline, interpretable |\n",
    "| ESM2-650M | 0.15‚Äì0.25 | Fast, good balance |\n",
    "| ESM2-3B | 0.18‚Äì0.28 | Higher accuracy |\n",
    "| ProtT5-XL | 0.15‚Äì0.25 | Alternative to ESM |\n",
    "| ProtAlbert | 0.08‚Äì0.15 | Lightweight option |\n",
    "\n",
    "**Recommended head models:** SVR (most stable), XGBoost (higher potential), KNN (simple baseline)\n",
    "\n",
    "### References\n",
    "- [ESM](https://github.com/facebookresearch/esm) ‚Äî Meta's protein language models\n",
    "- [ProtTrans](https://github.com/agemagician/ProtTrans) ‚Äî Protein transformer embeddings\n",
    "- [PLMC](https://github.com/debbiemarkslab/plmc) ‚Äî Evolutionary coupling analysis\n",
    "- [ProteinMCP](https://github.com/charlesxu90/ProteinMCP) ‚Äî This project"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "protein-mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}