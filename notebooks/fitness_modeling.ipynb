{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title_header"
   },
   "source": [
    "# ProteinMCP ‚Äî Fitness Modeling Workflow\n",
    "\n",
    "Build and compare protein fitness prediction models using multiple backbone architectures:\n",
    "\n",
    "| Model | Description |\n",
    "|-------|-------------|\n",
    "| **EV+OneHot** | Evolutionary couplings + one-hot encoding (PLMC) |\n",
    "| **ESM2-650M / ESM2-3B** | Meta's protein language models |\n",
    "| **ProtT5-XL / ProtAlbert** | ProtTrans transformer embeddings |\n",
    "\n",
    "Each model is trained with SVR, XGBoost, and KNN heads, then compared via 5-fold cross-validated Spearman correlation.\n",
    "\n",
    "**Prerequisites:** Docker, Claude Code CLI, ProteinMCP installed locally.\n",
    "\n",
    "**Links:** [GitHub](https://github.com/charlesxu90/ProteinMCP) ¬∑ [ESM](https://github.com/facebookresearch/esm) ¬∑ [ProtTrans](https://github.com/agemagician/ProtTrans) ¬∑ [PLMC](https://github.com/debbiemarkslab/plmc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ User Configuration ‚îÄ‚îÄ\n",
    "PROTEIN_NAME = \"TEVp_S219V\"\n",
    "use_example_data = True\n",
    "\n",
    "# Optional: set API key here or in .env. If unset, Claude CLI uses your logged-in account.\n",
    "ANTHROPIC_API_KEY = \"\"\n",
    "CLAUDE_MODEL = \"claude-sonnet-4-6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import utility and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "user_config"
   },
   "outputs": [],
   "source": "import os\nimport subprocess\nimport json\nimport select\n\n# ---------- Streaming command runner ----------\ndef run_cmd(cmd, cwd=None):\n    \"\"\"Run a shell command and stream stdout/stderr line-by-line in real time.\"\"\"\n    proc = subprocess.Popen(\n        cmd, shell=True, cwd=cwd,\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n        bufsize=1, text=True,\n    )\n    for line in proc.stdout:\n        print(line, end=\"\", flush=True)\n    proc.wait()\n    if proc.returncode != 0:\n        print(f\"\\n‚ö†Ô∏è  Command exited with code {proc.returncode}\")\n    return proc.returncode\n\n# ---------- Claude streaming helper ----------\ndef _display_claude_line(line):\n    \"\"\"Parse a single stream-json line from Claude CLI and print progress.\"\"\"\n    if not line.strip():\n        return\n    try:\n        data = json.loads(line)\n        msg_type = data.get('type', '')\n        subtype = data.get('subtype', '')\n\n        if msg_type == 'system':\n            if subtype == 'init':\n                session_id = data.get('session_id', '')[:8]\n                print(f\"  ü§ñ Session started: {session_id}...\", flush=True)\n            elif subtype != 'transcript':\n                print(f\"  ‚öôÔ∏è  System: {subtype}\", flush=True)\n\n        elif msg_type == 'assistant':\n            message = data.get('message', {})\n            for block in message.get('content', []):\n                block_type = block.get('type', '')\n                if block_type == 'thinking':\n                    text = block.get('thinking', '')[:100]\n                    print(f\"  üí≠ Thinking: {text}...\", flush=True)\n                elif block_type == 'text':\n                    lines = block.get('text', '').strip().split('\\n')\n                    for tl in lines[:5]:\n                        if tl.strip():\n                            print(f\"  {tl}\", flush=True)\n                    if len(lines) > 5:\n                        print(f\"  ... ({len(lines) - 5} more lines)\", flush=True)\n                elif block_type == 'tool_use':\n                    tool_name = block.get('name', 'unknown')\n                    tool_input = block.get('input', {})\n                    if tool_name == 'Bash':\n                        print(f\"  üîß Bash: {tool_input.get('command', '')[:80]}\", flush=True)\n                    elif tool_name in ('Read', 'Write', 'Edit'):\n                        print(f\"  üìñ {tool_name}: {tool_input.get('file_path', '')}\", flush=True)\n                    elif tool_name.startswith('mcp__'):\n                        print(f\"  üîå MCP: {tool_name}\", flush=True)\n                    else:\n                        print(f\"  üîß {tool_name}\", flush=True)\n\n        elif msg_type == 'user':\n            for block in data.get('message', {}).get('content', []):\n                if block.get('type') == 'tool_result':\n                    if block.get('is_error', False):\n                        err = block.get('content', '')\n                        err = err[:100] if isinstance(err, str) else str(err)[:100]\n                        print(f\"  ‚ùå Error: {err}\", flush=True)\n                    else:\n                        content = block.get('content', '')\n                        if isinstance(content, str) and content.strip():\n                            first = content.strip().split('\\n')[0][:80]\n                            if first:\n                                print(f\"  ‚úÖ Result: {first}\", flush=True)\n                        else:\n                            print(f\"  ‚úÖ Done\", flush=True)\n\n        elif msg_type == 'result':\n            if subtype == 'success':\n                print(f\"  ‚úÖ Completed successfully\", flush=True)\n            elif subtype == 'error':\n                print(f\"  ‚ùå Error: {data.get('error', 'Unknown')}\", flush=True)\n\n    except json.JSONDecodeError:\n        if line.strip():\n            print(f\"  {line}\", flush=True)\n\n\ndef run_claude(prompt, allowed_tools=None, cwd=None, model=None):\n    \"\"\"Run Claude CLI with real-time streaming output.\n\n    Args:\n        prompt: The prompt text to send to Claude (passed via stdin).\n        allowed_tools: Comma-separated tool names, e.g. \"Bash,Read,Write\".\n        cwd: Working directory for the claude process.\n        model: Model name override. Defaults to CLAUDE_MODEL global.\n\n    Returns:\n        Process return code (0 = success).\n    \"\"\"\n    _model = model or CLAUDE_MODEL\n    cmd = [\n        \"claude\",\n        \"--model\", _model,\n        \"-p\", \"-\",\n        \"--output-format\", \"stream-json\",\n        \"--verbose\",\n        \"--dangerously-skip-permissions\",\n    ]\n    if allowed_tools:\n        cmd += [\"--allowedTools\", allowed_tools]\n\n    print(f\"  ü§ñ Claude model: {_model}\")\n    print(f\"  üìã Tools: {allowed_tools or 'all'}\")\n    print(f\"  \" + \"-\" * 58)\n\n    proc = subprocess.Popen(\n        cmd, cwd=cwd,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True, bufsize=1,\n    )\n    proc.stdin.write(prompt)\n    proc.stdin.close()\n\n    while True:\n        if proc.poll() is not None:\n            # Drain remaining output\n            for line in (proc.stdout.read() or '').split('\\n'):\n                _display_claude_line(line)\n            for line in (proc.stderr.read() or '').split('\\n'):\n                if line.strip():\n                    print(f\"  ‚öôÔ∏è  {line}\", flush=True)\n            break\n\n        try:\n            readable, _, _ = select.select([proc.stdout, proc.stderr], [], [], 0.1)\n        except (ValueError, OSError):\n            break\n\n        for stream in readable:\n            line = stream.readline()\n            if line:\n                if stream == proc.stdout:\n                    _display_claude_line(line.rstrip('\\n'))\n                else:\n                    if line.strip():\n                        print(f\"  ‚öôÔ∏è  {line.rstrip()}\", flush=True)\n\n    rc = proc.wait()\n    print(f\"  \" + \"-\" * 58)\n    if rc != 0:\n        print(f\"  ‚ö†Ô∏è  Claude exited with code {rc}\")\n    return rc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and verify the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No API key set ‚Äî Claude CLI will use your logged-in account.\n",
      "Will use bundled example data for TEVp_S219V\n",
      "\n",
      "CLAUDE_MODEL : claude-sonnet-4-6\n",
      "PROTEIN_NAME : TEVp_S219V\n",
      "REPO_DIR     : /home/xux/Desktop/AgentMCP/ProteinMCP\n",
      "DATA_DIR     : /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V\n",
      "RESULTS_DIR  : /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V\n",
      "WT_FASTA     : /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/wt.fasta\n",
      "DATA_CSV     : /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/data.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- Paths ----------\n",
    "# Auto-detect REPO_DIR: walk up from notebook location to find project root\n",
    "_nb_dir = os.path.abspath(\"\")\n",
    "if os.path.basename(_nb_dir) == \"notebooks\":\n",
    "    REPO_DIR = os.path.dirname(_nb_dir)\n",
    "else:\n",
    "    REPO_DIR = _nb_dir\n",
    "\n",
    "# ---------- Load API key from .env if not set ----------\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    _env_file = os.path.join(REPO_DIR, \".env\")\n",
    "    if os.path.exists(_env_file):\n",
    "        with open(_env_file) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"ANTHROPIC_API_KEY=\"):\n",
    "                    ANTHROPIC_API_KEY = line.split(\"=\", 1)[1].strip().strip(\"\\\"'\")\n",
    "                    break\n",
    "        if ANTHROPIC_API_KEY:\n",
    "            print(f\"Loaded ANTHROPIC_API_KEY from {_env_file}\")\n",
    "\n",
    "if ANTHROPIC_API_KEY:\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "    print(f\"Using API key (ends with ...{ANTHROPIC_API_KEY[-4:]})\")\n",
    "else:\n",
    "    print(\"No API key set ‚Äî Claude CLI will use your logged-in account.\")\n",
    "\n",
    "# ---------- Data paths ----------\n",
    "DATA_DIR    = os.path.join(REPO_DIR, \"data\", PROTEIN_NAME)\n",
    "RESULTS_DIR = os.path.join(REPO_DIR, \"results\", PROTEIN_NAME)\n",
    "\n",
    "WT_FASTA = os.path.join(DATA_DIR, \"wt.fasta\")\n",
    "DATA_CSV = os.path.join(DATA_DIR, \"data.csv\")\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "if use_example_data:\n",
    "    print(f\"Will use bundled example data for {PROTEIN_NAME}\")\n",
    "else:\n",
    "    print(f\"Please place your wt.fasta and data.csv files in:\\n  {DATA_DIR}\")\n",
    "    input(\"Press Enter once the files are in place...\")\n",
    "    assert os.path.exists(WT_FASTA), f\"Missing {WT_FASTA} ‚Äî please provide wt.fasta\"\n",
    "    assert os.path.exists(DATA_CSV),  f\"Missing {DATA_CSV} ‚Äî please provide data.csv\"\n",
    "\n",
    "print(f\"\\nCLAUDE_MODEL : {CLAUDE_MODEL}\")\n",
    "print(f\"PROTEIN_NAME : {PROTEIN_NAME}\")\n",
    "print(f\"REPO_DIR     : {REPO_DIR}\")\n",
    "print(f\"DATA_DIR     : {DATA_DIR}\")\n",
    "print(f\"RESULTS_DIR  : {RESULTS_DIR}\")\n",
    "print(f\"WT_FASTA     : {WT_FASTA}\")\n",
    "print(f\"DATA_CSV     : {DATA_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install & Register MCPs\n",
    "\n",
    "> **Tip ‚Äî Local Docker builds:** Some Docker MCP images (e.g. `esm_mcp`) are large and may\n",
    "> time out during `pmcp install`. You can build and register them locally instead:\n",
    "> ```bash\n",
    "> cd tool-mcps/esm_mcp && docker build -t esm_mcp:latest . && cd ../..\n",
    "> claude mcp add esm_mcp -- docker run -i --rm --gpus all --ipc=host -v $PWD:$PWD esm_mcp:latest\n",
    "> ```\n",
    "> Replace `esm_mcp` with any other Docker MCP name (`plmc_mcp`, `ev_onehot_mcp`, `prottrans_mcp`) as needed.\n",
    "> The install cell below will skip MCPs that are already registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xux/miniforge3/envs/protein-mcp/bin/pmcp\n",
      "ProteinMCP already installed.\n",
      "/home/xux/.local/bin/claude\n",
      "Claude Code already installed.\n",
      "Docker version 29.2.1, build a5c7197\n",
      "Docker found.\n",
      "ProteinMCP & Claude Code ready.\n",
      "\n",
      "Elapsed: 0.0s\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "# Install ProteinMCP if not already present\n",
    "if run_cmd(\"which pmcp\") != 0:\n",
    "    run_cmd(f\"pip install -e {REPO_DIR}\")\n",
    "    run_cmd(f\"pip install -r {REPO_DIR}/requirements.txt\")\n",
    "else:\n",
    "    print(\"ProteinMCP already installed.\")\n",
    "\n",
    "# Install Claude Code if not already present\n",
    "if run_cmd(\"which claude\") != 0:\n",
    "    run_cmd(\"npm install -g @anthropic-ai/claude-code\")\n",
    "else:\n",
    "    print(\"Claude Code already installed.\")\n",
    "\n",
    "# Verify Docker is available (required for Docker MCPs)\n",
    "if run_cmd(\"docker --version\") != 0:\n",
    "    print(\"WARNING: Docker not found. Docker MCPs will not work.\")\n",
    "else:\n",
    "    print(\"Docker found.\")\n",
    "\n",
    "print(\"ProteinMCP & Claude Code ready.\")\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All MCPs already registered. Skipping installation.\n",
      "\n",
      "==================================================\n",
      "Checking MCP server health...\n",
      "\n",
      "msa_mcp: python /mnt/data/done_projects/2026/BioMolMCP/ProteinMCP/tool-mcps/msa_mcp/src/server.py - ‚úì Connected\n",
      "bindcraft_mcp: docker run -i --rm --user 129052:1129052 --gpus all --ipc=host -v /home/xux/Desktop/AgentMCP/ProteinMCP:/home/xux/Desktop/AgentMCP/ProteinMCP bindcraft_mcp:latest - ‚úì Connected\n",
      "plmc_mcp: docker run -i --rm --user 129052:1129052 --gpus all --ipc=host -v /home/xux/Desktop/AgentMCP/ProteinMCP:/home/xux/Desktop/AgentMCP/ProteinMCP plmc_mcp:latest - ‚úì Connected\n",
      "ev_onehot_mcp: docker run -i --rm --user 129052:1129052 --gpus all --ipc=host -v /home/xux/Desktop/AgentMCP/ProteinMCP:/home/xux/Desktop/AgentMCP/ProteinMCP ev_onehot_mcp:latest - ‚úì Connected\n",
      "prottrans_mcp: docker run -i --rm --user 129052:1129052 --gpus all --ipc=host -e HOME=/home/xux -v /home/xux/Desktop/AgentMCP/ProteinMCP:/home/xux/Desktop/AgentMCP/ProteinMCP -v /home/xux/.cache:/home/xux/.cache prottrans_mcp:latest - ‚úì Connected\n",
      "esm_mcp: docker run -i --rm --user 129052:1129052 --gpus all --ipc=host -e HOME=/home/xux -v /home/xux/Desktop/AgentMCP/ProteinMCP:/home/xux/Desktop/AgentMCP/ProteinMCP -v /home/xux/.cache:/home/xux/.cache esm_mcp:latest - ‚úì Connected\n",
      "\n",
      "Elapsed: 26.3s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "_t0 = time.time()\n",
    "\n",
    "# Required MCPs for fitness modeling workflow\n",
    "REQUIRED_MCPS = [\"msa_mcp\", \"plmc_mcp\", \"ev_onehot_mcp\", \"esm_mcp\", \"prottrans_mcp\"]\n",
    "\n",
    "# Check which MCPs are already registered\n",
    "_registered = subprocess.run([\"claude\", \"mcp\", \"list\"], capture_output=True, text=True).stdout\n",
    "_to_install = [m for m in REQUIRED_MCPS if m not in _registered]\n",
    "\n",
    "if _to_install:\n",
    "    print(f\"Installing {len(_to_install)} MCPs: {_to_install}\")\n",
    "    _skipped = [m for m in REQUIRED_MCPS if m in _registered]\n",
    "    if _skipped:\n",
    "        print(f\"Skipping already registered: {_skipped}\")\n",
    "\n",
    "    def _install_mcp(mcp_name):\n",
    "        \"\"\"Install a single MCP, capturing output for parallel execution.\"\"\"\n",
    "        result = subprocess.run(\n",
    "            [\"pmcp\", \"install\", mcp_name],\n",
    "            capture_output=True, text=True, cwd=REPO_DIR, timeout=1800\n",
    "        )\n",
    "        return mcp_name, result.returncode, result.stdout, result.stderr\n",
    "\n",
    "    # Install (pull Docker images) and register MCPs in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(_to_install)) as executor:\n",
    "        futures = {executor.submit(_install_mcp, name): name for name in _to_install}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            name, rc, stdout, stderr = future.result()\n",
    "            status = \"OK\" if rc == 0 else \"FAILED\"\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"  {name}: {status}\")\n",
    "            # Show last few meaningful lines\n",
    "            for line in stdout.strip().splitlines()[-5:]:\n",
    "                if line.strip():\n",
    "                    print(f\"    {line.strip()}\")\n",
    "            if rc != 0 and stderr:\n",
    "                print(f\"    stderr: {stderr.strip()[:200]}\")\n",
    "else:\n",
    "    print(\"All MCPs already registered. Skipping installation.\")\n",
    "\n",
    "# Verify MCP status\n",
    "print(f\"\\n{'='*50}\")\n",
    "run_cmd(\"claude mcp list\", cwd=REPO_DIR)\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 ‚Äî Setup Results Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/wt.fasta ‚Üí /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/wt.fasta\n",
      "Copied /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/data.csv ‚Üí /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/data.csv\n",
      "\n",
      "Results directory ready: /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V\n",
      "Files: ['data.csv', 'wt.fasta']\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "EXAMPLE_DIR = os.path.join(REPO_DIR, \"examples\", \"case1_fitness_modeling\")\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "if use_example_data:\n",
    "    # Copy bundled example data into DATA_DIR\n",
    "    for fname in [\"wt.fasta\", \"data.csv\"]:\n",
    "        src = os.path.join(EXAMPLE_DIR, fname)\n",
    "        dst = os.path.join(DATA_DIR, fname)\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copy2(src, dst)\n",
    "            print(f\"Copied {src} ‚Üí {dst}\")\n",
    "\n",
    "# Copy input files to RESULTS_DIR (needed by training tools)\n",
    "for fname in [\"wt.fasta\", \"data.csv\"]:\n",
    "    src = os.path.join(DATA_DIR, fname)\n",
    "    dst = os.path.join(RESULTS_DIR, fname)\n",
    "    if not os.path.exists(dst):\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"Copied {src} ‚Üí {dst}\")\n",
    "\n",
    "# Verify\n",
    "assert os.path.exists(os.path.join(RESULTS_DIR, \"wt.fasta\")), \"wt.fasta missing in RESULTS_DIR\"\n",
    "assert os.path.exists(os.path.join(RESULTS_DIR, \"data.csv\")),  \"data.csv missing in RESULTS_DIR\"\n",
    "print(f\"\\nResults directory ready: {RESULTS_DIR}\")\n",
    "print(f\"Files: {os.listdir(RESULTS_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 ‚Äî Generate MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "step1_msa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ü§ñ Claude model: claude-sonnet-4-6\n",
      "  üìã Tools: mcp__msa_mcp__generate_msa,Bash,Read,Write\n",
      "  ----------------------------------------------------------\n",
      "  ü§ñ Session started: fbd297c9...\n",
      "  üí≠ Thinking: Let me read the FASTA file first to get the protein sequence....\n",
      "  üìñ Read: /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/wt.fasta\n",
      "  ‚úÖ Result: 1‚Üí>TEVp_S219V\n",
      "  üîß Bash: mkdir -p /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V\n",
      "  ‚úÖ Done\n",
      "  üîå MCP: mcp__msa_mcp__generate_msa\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"ticket_id\":\"iyy0MMnm2QhYJwooxY4iERYXJRNBSBzDMyy8Fg\",\"output\n",
      "  MSA generated successfully. Here's a summary:\n",
      "  - **Output file:** `/home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/TEVp_S219V.a3m`\n",
      "  - **Sequence length:** 236 amino acids\n",
      "  - **MSA depth:** 472 sequences\n",
      "  ... (1 more lines)\n",
      "  ‚úÖ Completed successfully\n",
      "  ----------------------------------------------------------\n",
      "\n",
      "MSA generated: /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/TEVp_S219V.a3m\n",
      "Elapsed: 35.6s\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "Can you obtain the MSA for {PROTEIN_NAME} from {WT_FASTA} using msa mcp \\\n",
    "and save it to {RESULTS_DIR}/{PROTEIN_NAME}.a3m.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt,\n",
    "    allowed_tools=\"mcp__msa_mcp__generate_msa,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# Verify output\n",
    "msa_file = f\"{RESULTS_DIR}/{PROTEIN_NAME}.a3m\"\n",
    "assert os.path.exists(msa_file), f\"MSA file not found: {msa_file}\"\n",
    "print(f\"\\nMSA generated: {msa_file}\")\n",
    "print(f\"Elapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 ‚Äî Build PLMC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "step2_plmc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ü§ñ Claude model: claude-sonnet-4-6\n",
      "  üìã Tools: mcp__plmc_mcp__plmc_convert_a3m_to_a2m,mcp__plmc_mcp__plmc_generate_model,Bash,Read,Write\n",
      "  ----------------------------------------------------------\n",
      "  ü§ñ Session started: b7792c48...\n",
      "  üí≠ Thinking: The user wants to build an EV model using the plmc MCP. The steps are:\n",
      "1. Convert A3M to A2M format\n",
      "...\n",
      "  I'll start by converting the A3M file to A2M format, then run plmc to generate the EV model.\n",
      "  üîå MCP: mcp__plmc_mcp__plmc_convert_a3m_to_a2m\n",
      "  ‚ùå Error: Error calling tool 'plmc_convert_a3m_to_a2m': Command '['/opt/conda/bin/reformat.pl', 'a3m', 'a2m', \n",
      "  The output directory doesn't exist yet. Let me create it first.\n",
      "  üîß Bash: mkdir -p /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/plmc\n",
      "  ‚úÖ Done\n",
      "  üîå MCP: mcp__plmc_mcp__plmc_convert_a3m_to_a2m\n",
      "  ‚úÖ Result: {\"message\":\"Successfully converted A3M to A2M format and cleaned query gaps\",\"re\n",
      "  A3M converted to A2M successfully. Now running plmc to generate the EV model.\n",
      "  üîå MCP: mcp__plmc_mcp__plmc_generate_model\n",
      "  ‚ùå Error: Error calling tool 'plmc_generate_model': Focus sequence identifier must be provided\n",
      "  I need to get the sequence ID from the FASTA file to use as the focus sequence.\n",
      "  üîß Bash: head -1 /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/wt.fasta\n",
      "  ‚úÖ Result: >TEVp_S219V\n",
      "  üîå MCP: mcp__plmc_mcp__plmc_generate_model\n",
      "  ‚úÖ Result: {\"message\":\"PLMC model generated successfully for TEVp_S219V\",\"reference\":\"https\n",
      "  The EV model has been built successfully. Here's a summary of what was generated in `/home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/plmc/`:\n",
      "  | File | Description |\n",
      "  |------|-------------|\n",
      "  | `TEVp_S219V.a2m` | Converted alignment (A3M ‚Üí A2M, query gaps removed) |\n",
      "  ... (10 more lines)\n",
      "  ‚úÖ Completed successfully\n",
      "  ----------------------------------------------------------\n",
      "Symlinked TEVp_S219V.model_params -> uniref100.model_params\n",
      "Symlinked TEVp_S219V.EC -> uniref100.EC\n",
      "\n",
      "PLMC model built: ['uniref100.model_params', 'TEVp_S219V.model_params', 'TEVp_S219V.a2m', 'uniref100.EC', 'TEVp_S219V.EC']\n",
      "Elapsed: 224.8s\n"
     ]
    }
   ],
   "source": [
    "import os, time, subprocess, glob as globmod\n",
    "_t0 = time.time()\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "I have created an a3m file in {RESULTS_DIR}/{PROTEIN_NAME}.a3m. \\\n",
    "Can you help build an EV model using plmc mcp and save it to {RESULTS_DIR}/plmc directory. \\\n",
    "The wild-type sequence is {WT_FASTA}.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt,\n",
    "    allowed_tools=\"mcp__plmc_mcp__plmc_convert_a3m_to_a2m,mcp__plmc_mcp__plmc_generate_model,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# Create symlinks: ev_onehot_mcp expects uniref100.model_params and uniref100.EC\n",
    "plmc_dir = f\"{RESULTS_DIR}/plmc\"\n",
    "\n",
    "# Fix ownership if directory is root-owned (Docker container artifact)\n",
    "if os.path.isdir(plmc_dir) and not os.access(plmc_dir, os.W_OK):\n",
    "    uid, gid = os.getuid(), os.getgid()\n",
    "    subprocess.run([\"sudo\", \"chown\", \"-R\", f\"{uid}:{gid}\", plmc_dir])\n",
    "\n",
    "# Find actual output files and create symlinks\n",
    "for suffix in [\"model_params\", \"EC\"]:\n",
    "    link_path = os.path.join(plmc_dir, f\"uniref100.{suffix}\")\n",
    "    if os.path.isfile(link_path):\n",
    "        continue  # Already exists as a real file\n",
    "    if os.path.islink(link_path) and os.path.exists(link_path):\n",
    "        continue  # Valid symlink already exists\n",
    "    # Remove broken symlink if present\n",
    "    if os.path.islink(link_path):\n",
    "        os.unlink(link_path)\n",
    "    # Find the actual file (e.g. TEVp_S219V.model_params)\n",
    "    candidates = [f for f in os.listdir(plmc_dir)\n",
    "                  if f.endswith(f\".{suffix}\") and f != f\"uniref100.{suffix}\"]\n",
    "    if candidates:\n",
    "        os.symlink(candidates[0], link_path)\n",
    "        print(f\"Symlinked {candidates[0]} -> uniref100.{suffix}\")\n",
    "\n",
    "# Verify outputs\n",
    "assert os.path.exists(f\"{plmc_dir}/uniref100.model_params\"), \"PLMC model_params symlink missing\"\n",
    "assert os.path.exists(f\"{plmc_dir}/uniref100.EC\"), \"PLMC EC symlink missing\"\n",
    "print(f\"\\nPLMC model built: {os.listdir(plmc_dir)}\")\n",
    "print(f\"Elapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äî Build EV+OneHot Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "step3_ev_onehot"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ü§ñ Claude model: claude-sonnet-4-6\n",
      "  üìã Tools: mcp__ev_onehot_mcp__ev_onehot_train_fitness_predictor,Bash,Read,Write\n",
      "  ----------------------------------------------------------\n",
      "  ü§ñ Session started: 9f12c339...\n",
      "  üí≠ Thinking: The user wants to train an EV+OneHot fitness prediction model using the ev_onehot_mcp tool. Let me c...\n",
      "  Let me first check the directory structure to understand what files are available.\n",
      "  üîß Bash: ls /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/ && echo \"---\" && ls\n",
      "  ‚úÖ Result: data.csv\n",
      "  üí≠ Thinking: Good, I can see the directory structure. The plmc directory contains:\n",
      "- TEVp_S219V.a2m\n",
      "- TEVp_S219V....\n",
      "  Let me check the ev_onehot_mcp source to understand the expected directory structure.\n",
      "  üîß Bash: find /home/xux/Desktop/AgentMCP/ProteinMCP/tool-mcps/ev_onehot_mcp -name \"*.py\" \n",
      "  ‚úÖ Done\n",
      "  üîß Bash: find /home/xux/Desktop/AgentMCP/ProteinMCP/tool-mcps -maxdepth 2 -name \"ev_oneho\n",
      "  ‚úÖ Result: /home/xux/Desktop/AgentMCP/ProteinMCP/tool-mcps/ev_onehot_mcp\n",
      "  üîß Bash: ls /home/xux/Desktop/AgentMCP/ProteinMCP/tool-mcps/ev_onehot_mcp/\n",
      "  ‚úÖ Result: CLAUDE.md\n",
      "  üîß Bash: find /home/xux/Desktop/AgentMCP/ProteinMCP/tool-mcps/ev_onehot_mcp/src -name \"*.\n",
      "  ‚úÖ Result: /home/xux/Desktop/AgentMCP/ProteinMCP/tool-mcps/ev_onehot_mcp/src/tools/predicto\n",
      "  üìñ Read: /home/xux/Desktop/AgentMCP/ProteinMCP/tool-mcps/ev_onehot_mcp/src/tools/train.py\n",
      "  ‚úÖ Result: 1‚Üí\"\"\"\n",
      "  üí≠ Thinking: Now I understand the expected directory structure. The tool expects:\n",
      "- `data_dir`: directory contain...\n",
      "  The tool expects `data_dir` to contain `data.csv`, `wt.fasta`, and a `plmc/` subfolder ‚Äî which matches exactly the structure in `/home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/`. Let me train the model now.\n",
      "  üîå MCP: mcp__ev_onehot_mcp__ev_onehot_train_fitness_predictor\n",
      "  ‚úÖ Result: {\"message\":\"5-fold CV: 0.261 ¬± 0.084, Final model trained on 384 samples\",\"artif\n",
      "  The EV+OneHot model has been trained successfully. Here's a summary:\n",
      "  **Results:**\n",
      "  - **5-fold CV Spearman**: 0.261 ¬± 0.084\n",
      "  - **Training samples**: 384\n",
      "  ... (6 more lines)\n",
      "  ‚úÖ Completed successfully\n",
      "  ----------------------------------------------------------\n",
      "\n",
      "EV+OneHot model trained. Metrics:\n",
      "           stage fold  n_train  n_test  spearman_correlation\n",
      "cross_validation    1      307      77              0.420947\n",
      "cross_validation    2      307      77              0.241468\n",
      "cross_validation    3      307      77              0.246750\n",
      "cross_validation    4      307      77              0.174835\n",
      "cross_validation    5      308      76              0.221365\n",
      "      cv_summary mean      307      76              0.261073\n",
      "      cv_summary  std        0       0              0.083861\n",
      "     final_model  all      384       0              0.572081\n",
      "\n",
      "Elapsed: 74.5s\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "I have created a plmc model in directory {RESULTS_DIR}/plmc. \\\n",
    "Can you help build an EV+OneHot model using ev_onehot_mcp and save it to {RESULTS_DIR}/ directory. \\\n",
    "The wild-type sequence is {RESULTS_DIR}/wt.fasta, and the dataset is {RESULTS_DIR}/data.csv.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt,\n",
    "    allowed_tools=\"mcp__ev_onehot_mcp__ev_onehot_train_fitness_predictor,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# Verify output\n",
    "assert os.path.exists(f\"{RESULTS_DIR}/metrics_summary.csv\"), \"EV+OneHot metrics not found\"\n",
    "print(f\"\\nEV+OneHot model trained. Metrics:\")\n",
    "import pandas as pd\n",
    "print(pd.read_csv(f\"{RESULTS_DIR}/metrics_summary.csv\").to_string(index=False))\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äî Build ESM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "step4_esm"
   },
   "outputs": [],
   "source": "import os, time\n_t0 = time.time()\n\n# ESM tool names for allowed_tools\n_ESM_TOOLS = \",\".join([\n    \"mcp__esm_mcp__esm_extract_embeddings_from_csv\",\n    \"mcp__esm_mcp__esm_train_fitness_model\",\n    \"Bash\", \"Read\", \"Write\",\n])\n\n# --- 4.1: ESM2-650M (extract embeddings + train svr/xgboost/knn) ---\nprint(\"=\"*60)\nprint(\"ESM2-650M: extracting embeddings & training models\")\nprint(\"=\"*60)\n\nprompt_650m = f\"\"\"\\\nCan you help train ESM models for data in {RESULTS_DIR}/ and save them to \\\n{RESULTS_DIR}/ESM2-650M_{{head_model}} using the esm_mcp server with svr, xgboost, \\\nand knn as the head models.\nPlease convert the relative path to absolute path before calling the MCP servers.\nExtract the embeddings first if they are not already created.\n\"\"\"\n\nrun_claude(prompt_650m, allowed_tools=_ESM_TOOLS, cwd=REPO_DIR)\n\n# --- 4.2: ESM2-3B (extract embeddings + train svr/xgboost/knn) ---\nprint(\"\\n\" + \"=\"*60)\nprint(\"ESM2-3B: extracting embeddings & training models\")\nprint(\"=\"*60)\n\nprompt_3b = f\"\"\"\\\nCan you help train ESM models for data in {RESULTS_DIR}/ and save them to \\\n{RESULTS_DIR}/ESM2-3B_{{head_model}} using the esm mcp server with svr, xgboost, \\\nand knn as the head models and esm2_t36_3B_UR50D as the backbone.\nPlease convert the relative path to absolute path before calling the MCP servers.\nExtract the embeddings first if they are not already created.\n\"\"\"\n\nrun_claude(prompt_3b, allowed_tools=_ESM_TOOLS, cwd=REPO_DIR)\n\n# Verify\nprint(\"\\n\" + \"=\"*60)\nprint(\"ESM model outputs:\")\nprint(\"=\"*60)\nfor backbone in [\"ESM2-650M\", \"ESM2-3B\"]:\n    for head in [\"svr\", \"xgboost\", \"knn\"]:\n        d = f\"{RESULTS_DIR}/{backbone}_{head}\"\n        if os.path.isdir(d):\n            print(f\"  ‚úì {backbone}_{head}: {os.listdir(d)}\")\n        else:\n            print(f\"  ‚úó {backbone}_{head}: NOT FOUND\")\n\nprint(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 ‚Äî Build ProtTrans Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "step5_prottrans"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ProtTrans: extracting embeddings & training models\n",
      "============================================================\n",
      "  ü§ñ Claude model: claude-haiku-4-5-20251001\n",
      "  üìã Tools: mcp__prottrans_mcp__prottrans_extract_embeddings,mcp__prottrans_mcp__prottrans_train_fitness_model,Bash,Read,Write\n",
      "  ----------------------------------------------------------\n",
      "  ü§ñ Session started: 8ef4f8a9...\n",
      "  üí≠ Thinking: The user wants me to:\n",
      "1. Train ProtTrans models for data in /home/xux/Desktop/AgentMCP/ProteinMCP/re...\n",
      "  I'll help you train ProtTrans models for the TEVp_S219V dataset. Let me start by exploring the data directory and then create embeddings and train models for all combinations.\n",
      "  üîß Bash: ls -la /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/\n",
      "  ‚ùå Error: Exit code 2\n",
      "ls: cannot access '/home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/': No such f\n",
      "  üí≠ Thinking: The directory doesn't exist. Let me check if the results directory exists at all....\n",
      "  üîß Bash: ls -la /home/xux/Desktop/AgentMCP/ProteinMCP/results/ 2>/dev/null || echo \"resul\n",
      "  ‚úÖ Result: total 28\n",
      "  üí≠ Thinking: The TEVp_S219V directory doesn't exist yet. The user might want me to create it or they may have a d...\n",
      "  The TEVp_S219V directory doesn't exist yet. Let me search for any relevant data files in the project:\n",
      "  üîß Glob\n",
      "  ‚úÖ Result: /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/data.csv\n",
      "  üîß Bash: find /home/xux/Desktop/AgentMCP/ProteinMCP -name \"*TEV*\" -o -name \"*S219V*\" 2>/d\n",
      "  ‚úÖ Result: /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V\n",
      "  üí≠ Thinking: Great! The data file is at /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/data.csv. The user ...\n",
      "  Perfect! I found the data file. Let me examine it and set up the training pipeline:\n",
      "  üìñ Read: /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/data.csv\n",
      "  ‚úÖ Result: 1‚ÜíAACombo,AA1,AA2,AA3,AA4,fitness,active,muts,n_mut,mutations,seq,n_muts,log_fit\n",
      "  üîß Bash: wc -l /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/data.csv && head -1 \n",
      "  ‚úÖ Result: 385 /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/data.csv\n",
      "  üí≠ Thinking: Good! The data has:\n",
      "- 385 rows (including header)\n",
      "- Columns: AACombo, AA1, AA2, AA3, AA4, fitness, a...\n",
      "  Now let me check if embeddings already exist and create the output directory:\n",
      "  üîß Bash: ls -la /home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_S219V/\n",
      "  ‚úÖ Result: total 144\n",
      "  üîß Bash: mkdir -p /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V && echo \"Outpu\n",
      "  ‚úÖ Result: Output directory created\n",
      "  üí≠ Thinking: Good. The embeddings don't exist yet (I only see data.csv and wt.fasta). Now I need to create embedd...\n",
      "  Now I'll create embeddings for both backbone models and train all combinations. Let me start by creating embeddings for ProtT5-XL:\n",
      "  üîß Skill\n",
      "  ‚úÖ Result: Launching skill: fitness-model\n",
      "  üí≠ Thinking: Now I understand the fitness-model skill and the complete workflow. Based on the user's request, I n...\n",
      "  Now I understand the workflow. Let me create ProtTrans embeddings for both backbone models and then train all 6 model combinations (2 backbones √ó 3 head models). Let me start:\n",
      "  üîß Bash: python3 << 'EOF'\n",
      "import sys\n",
      "sys.path.insert(0, '/home/xux/Desktop/AgentMCP/Prote\n",
      "  ‚úÖ Result: [TIMING] Started step: ProtTrans at 16:06:25\n",
      "  üí≠ Thinking: Good, now let me create embeddings for ProtT5-XL and ProtAlbert. I'll call the prottrans_extract_emb...\n",
      "  Now let me create embeddings for both ProtT5-XL and ProtAlbert:\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_extract_embeddings\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_extract_embeddings\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"csv_path\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"csv_path\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/data/TEVp_\n",
      "  üí≠ Thinking: Great! Both embeddings have been created successfully:\n",
      "- ProtT5-XL: 384 sequences with 1024-dimensio...\n",
      "  Excellent! Both embeddings created. Now let me copy the data files to the results directory and train all 6 model combinations:\n",
      "  üîß Bash: \n",
      "# Copy input files to results directory\n",
      "cp /home/xux/Desktop/AgentMCP/ProteinMC\n",
      "  ‚úÖ Result: Files copied to results directory\n",
      "  üí≠ Thinking: Now I need to handle the embeddings location. The embeddings were created in the data directory, but...\n",
      "  üîß Bash: \n",
      "# Copy embeddings to results directory\n",
      "cp -r /home/xux/Desktop/AgentMCP/Protein\n",
      "  ‚úÖ Result: Embeddings copied to results directory\n",
      "  üí≠ Thinking: Good. Now I'll train all 6 model combinations. Let me call the training tool for each combination. T...\n",
      "  Now I'll train all 6 model combinations in parallel. This may take some time:\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_train_fitness_model\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_train_fitness_model\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_train_fitness_model\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_train_fitness_model\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_train_fitness_model\n",
      "  üîå MCP: mcp__prottrans_mcp__prottrans_train_fitness_model\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  ‚úÖ Result: {\"status\":\"success\",\"output_dir\":\"/home/xux/Desktop/AgentMCP/ProteinMCP/results/\n",
      "  üí≠ Thinking: Good! All 6 models have completed training. I notice that the ProtT5-XL models have CV results (mean...\n",
      "  Great! All 6 models have been trained. I see the ProtT5-XL models completed with CV scores, but the ProtAlbert models show null values. Let me check the summary files and create a results overview:\n",
      "  üîß Bash: \n",
      "python3 << 'EOF'\n",
      "import pandas as pd\n",
      "from pathlib import Path\n",
      "\n",
      "results_dir = Pa\n",
      "  ‚úÖ Result: ======================================================================\n",
      "  üí≠ Thinking: I see that the ProtAlbert models didn't generate valid CV results. Let me check the CV results files...\n",
      "  üìñ Read: /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/ProtAlbert_svr/ProtAlbert_svr_cv_results.csv\n",
      "  ‚úÖ Result: 1‚Üífold,train_spearman,test_spearman,train_size,test_size\n",
      "  üìñ Read: /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/ProtAlbert_svr/training_summary.csv\n",
      "  ‚úÖ Result: 1‚Üístatus,output_dir,cv_results,cv_mean,cv_std,cv_min,cv_max,cv_results_file,n_sa\n",
      "  üí≠ Thinking: The ProtAlbert models returned NaN values for the Spearman correlations. This is likely because the ...\n",
      "  The ProtAlbert models encountered issues (returning NaN for CV scores), likely due to the high-dimensional embeddings (4096 dims). However, the ProtT5-XL models trained successfully. Let me complete the workflow and create a final summary:\n",
      "  üîß Bash: python3 << 'EOF'\n",
      "import sys\n",
      "sys.path.insert(0, '/home/xux/Desktop/AgentMCP/Prote\n",
      "  ‚úÖ Result: [TIMING] Completed step: ProtTrans in 1.37 minutes\n",
      "  üí≠ Thinking: Good. Now let me create a comprehensive final report summarizing what was done and the results....\n",
      "  Perfect! Let me create a final comprehensive summary report:\n",
      "  üîß Bash: \n",
      "cat > /home/xux/Desktop/AgentMCP/ProteinMCP/results/TEVp_S219V/TRAINING_SUMMARY\n",
      "  ‚úÖ Result: ================================================================================\n",
      "  üí≠ Thinking: Perfect! Now let me create a final summary showing the directory structure and what was accomplished...\n",
      "  Excellent! Let me verify the final output structure:\n",
      "  üîß Bash: \n",
      "echo \"=== Final Results Directory Structure ===\" && \\\n",
      "ls -lh /home/xux/Desktop/\n",
      "  ‚úÖ Result: === Final Results Directory Structure ===\n",
      "  üí≠ Thinking: Perfect! Everything has been successfully created. Let me create one more summary showing what was a...\n",
      "  Perfect! Here's the final summary:\n",
      "  ## ‚úÖ ProtTrans Model Training Complete\n",
      "  ### Summary\n",
      "  ... (36 more lines)\n",
      "  ‚úÖ Completed successfully\n",
      "  ----------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "ProtTrans model outputs:\n",
      "============================================================\n",
      "  ‚úì ProtT5-XL_svr: ['ProtT5-XL_svr_cv_results.csv', 'final_model', 'training_summary.csv', 'cv_folds']\n",
      "  ‚úì ProtT5-XL_xgboost: ['ProtT5-XL_xgboost_cv_results.csv', 'final_model', 'training_summary.csv', 'cv_folds']\n",
      "  ‚úì ProtT5-XL_knn: ['final_model', 'training_summary.csv', 'ProtT5-XL_knn_cv_results.csv', 'cv_folds']\n",
      "  ‚úì ProtAlbert_svr: ['ProtAlbert_svr_cv_results.csv', 'final_model', 'training_summary.csv', 'cv_folds']\n",
      "  ‚úì ProtAlbert_xgboost: ['ProtAlbert_xgboost_cv_results.csv', 'final_model', 'training_summary.csv', 'cv_folds']\n",
      "  ‚úì ProtAlbert_knn: ['ProtAlbert_knn_cv_results.csv', 'final_model', 'training_summary.csv', 'cv_folds']\n",
      "\n",
      "Elapsed: 163.0s\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "_t0 = time.time()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ProtTrans: extracting embeddings & training models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "Can you help train ProtTrans models for data in {RESULTS_DIR}/ and save them to \\\n",
    "{RESULTS_DIR}/{{backbone_model}}_{{head_model}} using the prottrans mcp server with \\\n",
    "ProtT5-XL and ProtAlbert as backbone_models and knn, xgboost, and svr as the head models.\n",
    "Please convert the relative path to absolute path before calling the MCP servers.\n",
    "Create the embeddings if they are not created.\n",
    "\"\"\"\n",
    "\n",
    "run_claude(\n",
    "    prompt,\n",
    "    allowed_tools=\"mcp__prottrans_mcp__prottrans_extract_embeddings,mcp__prottrans_mcp__prottrans_train_fitness_model,Bash,Read,Write\",\n",
    "    cwd=REPO_DIR,\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ProtTrans model outputs:\")\n",
    "print(\"=\"*60)\n",
    "for backbone in [\"ProtT5-XL\", \"ProtAlbert\"]:\n",
    "    for head in [\"svr\", \"xgboost\", \"knn\"]:\n",
    "        d = f\"{RESULTS_DIR}/{backbone}_{head}\"\n",
    "        if os.path.isdir(d):\n",
    "            print(f\"  ‚úì {backbone}_{head}: {os.listdir(d)}\")\n",
    "        else:\n",
    "            print(f\"  ‚úó {backbone}_{head}: NOT FOUND\")\n",
    "\n",
    "print(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 ‚Äî Aggregate Results & Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "troubleshooting"
   },
   "outputs": [],
   "source": "import os, time\nimport pandas as pd\n_t0 = time.time()\n\n# ---- 6.1 Collect and aggregate all model results ----\nresults = []\n\n# EV+OneHot ‚Äî metrics_summary.csv (stage/fold format)\nev_path = os.path.join(RESULTS_DIR, \"metrics_summary.csv\")\nif os.path.exists(ev_path):\n    ev = pd.read_csv(ev_path)\n    cv_mean = ev[ev[\"fold\"] == \"mean\"][\"spearman_correlation\"].values[0]\n    cv_std  = ev[ev[\"fold\"] == \"std\"][\"spearman_correlation\"].values[0]\n    results.append({\"backbone\": \"EV+OneHot\", \"head\": \"ridge\",\n                    \"mean_cv_spearman\": cv_mean, \"std_cv_spearman\": cv_std})\nelse:\n    print(f\"Warning: EV+OneHot metrics not found at {ev_path}\")\n\n# ESM & ProtTrans ‚Äî training_summary.csv in subdirectories\nfor dir_name in sorted(os.listdir(RESULTS_DIR)):\n    summary = os.path.join(RESULTS_DIR, dir_name, \"training_summary.csv\")\n    if not os.path.exists(summary):\n        continue\n    df = pd.read_csv(summary)\n    if \"mean_cv_spearman\" in df.columns:\n        mean_sp = df[\"mean_cv_spearman\"].values[0]\n        std_sp  = df[\"std_cv_spearman\"].values[0]\n    elif \"cv_mean\" in df.columns:\n        mean_sp = df[\"cv_mean\"].values[0]\n        std_sp  = df[\"cv_std\"].values[0]\n    else:\n        continue\n    # Skip NaN results (e.g. ProtAlbert can produce NaN with small datasets)\n    if pd.isna(mean_sp):\n        print(f\"  Skipping {dir_name} (NaN CV results)\")\n        continue\n    parts = dir_name.rsplit(\"_\", 1)\n    if len(parts) == 2:\n        results.append({\"backbone\": parts[0], \"head\": parts[1],\n                        \"mean_cv_spearman\": mean_sp, \"std_cv_spearman\": std_sp})\n\nall_models = pd.DataFrame(results).sort_values(\"mean_cv_spearman\", ascending=False)\nall_models.to_csv(os.path.join(RESULTS_DIR, \"all_models_comparison.csv\"), index=False)\nprint(f\"\\nSaved {len(results)} model results to all_models_comparison.csv\\n\")\nprint(all_models.to_string(index=False))\n\nif len(results) == 0:\n    print(\"\\n‚ö†Ô∏è  No model results found. Check that Steps 3-5 completed successfully.\")\nelse:\n    # ---- 6.2 Generate four-panel visualization ----\n    VIZ_SCRIPT = os.path.join(REPO_DIR, \"workflow-skills\", \"scripts\", \"fitness_modeling_viz.py\")\n\n    # Install viz deps if needed, then run with system python\n    run_cmd(\"pip install -q matplotlib seaborn scipy Pillow\")\n    run_cmd(f\"python {VIZ_SCRIPT} {RESULTS_DIR}\")\n\n    # ---- 6.3 Display figure inline ----\n    from IPython.display import display, Image\n\n    summary_png = os.path.join(RESULTS_DIR, \"figures\", \"fitness_modeling_summary.png\")\n    if os.path.exists(summary_png):\n        print(\"\\nFour-panel summary:\")\n        display(Image(filename=summary_png, width=800))\n    else:\n        # Try individual figures\n        figs_dir = os.path.join(RESULTS_DIR, \"figures\")\n        if os.path.isdir(figs_dir):\n            for f in sorted(os.listdir(figs_dir)):\n                if f.endswith(\".png\"):\n                    print(f\"\\n{f}:\")\n                    display(Image(filename=os.path.join(figs_dir, f), width=500))\n        else:\n            print(\"No figures generated ‚Äî check logs above.\")\n\nprint(f\"\\nElapsed: {time.time() - _t0:.1f}s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Instructions & Troubleshooting\n",
    "\n",
    "### Data Format\n",
    "Your `data.csv` must contain at minimum:\n",
    "- **`seq`** ‚Äî Full protein sequence\n",
    "- **`log_fitness`** ‚Äî Log-transformed fitness value (target)\n",
    "\n",
    "Your `wt.fasta` should contain the wild-type reference sequence in standard FASTA format.\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| `uniref100.model_params not found` | Re-run Step 2 ‚Äî symlinks may not have been created |\n",
    "| `wt.fasta not found` in EV+OneHot | Ensure wt.fasta is in RESULTS_DIR (Step 0) |\n",
    "| ESM embeddings extraction fails | Check that `esm_mcp` Docker image is pulled (`pmcp status`) |\n",
    "| GPU Out of Memory | Try smaller backbone (ESM2-650M instead of 3B) |\n",
    "| Low Spearman correlation | Check data quality; ensure proper log-transformation |\n",
    "| MCP not found | Run `pmcp install <mcp_name>` to pull the Docker image and register |\n",
    "| Docker image not pulled | Run `pmcp install <mcp_name>` to pull the image |\n",
    "\n",
    "### Model Performance Reference\n",
    "\n",
    "| Model | Typical CV Spearman | Best Use |\n",
    "|-------|-------------------|----------|\n",
    "| EV+OneHot | 0.20‚Äì0.35 | Baseline, interpretable |\n",
    "| ESM2-650M | 0.15‚Äì0.25 | Fast, good balance |\n",
    "| ESM2-3B | 0.18‚Äì0.28 | Higher accuracy |\n",
    "| ProtT5-XL | 0.15‚Äì0.25 | Alternative to ESM |\n",
    "| ProtAlbert | 0.08‚Äì0.15 | Lightweight option |\n",
    "\n",
    "**Recommended head models:** SVR (most stable), XGBoost (higher potential), KNN (simple baseline)\n",
    "\n",
    "### References\n",
    "- [ESM](https://github.com/facebookresearch/esm) ‚Äî Meta's protein language models\n",
    "- [ProtTrans](https://github.com/agemagician/ProtTrans) ‚Äî Protein transformer embeddings\n",
    "- [PLMC](https://github.com/debbiemarkslab/plmc) ‚Äî Evolutionary coupling analysis\n",
    "- [ProteinMCP](https://github.com/charlesxu90/ProteinMCP) ‚Äî This project"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "protein-mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}